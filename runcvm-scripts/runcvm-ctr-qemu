#!/.runcvm/guest/bin/bash

# Exit on errors
set -o errexit -o pipefail

# Load original environment
. /.runcvm/config

# Load defaults
. $RUNCVM_GUEST/scripts/runcvm-ctr-defaults && unset PATH

QEMU_IFUP="$RUNCVM_GUEST/scripts/runcvm-ctr-qemu-ifup"
QEMU_IFDOWN="$RUNCVM_GUEST/scripts/runcvm-ctr-qemu-ifdown"

INIT="init=$RUNCVM_GUEST/scripts/runcvm-vm-init"

# Must export TERMINFO so curses library can find terminfo database.
export TERMINFO="$RUNCVM_GUEST/usr/share/terminfo"

error() {
  echo "$1" >&2
  exit 1
}

# Argument e.g. /volume/disk1,/var/lib/docker,ext4,5G
do_disk() {
  local spec="$1"
  local id="$2"
  local src dst fs size dir UUID queues

  local IFS=','
  read src dst fs size <<< $(echo "$spec")

  if [[ -z "$src" || -z "$dst" || -z "$fs" ]]; then
    error "Error: disk spec '$spec' invalid: src, dst and fs must all be specified"
  fi

  if [[ "$src" = "$dst" ]]; then
    error "Error: disk spec '$spec' invalid: src '$src' cannot be same as dst"
  fi

  if [[ -e "$src" && ! -f "$src" ]]; then
    error "Error: disk spec '$spec' invalid: src '$src' must be a plain file if it exists"
  fi

  if [[ -e "$dst" && ! -d "$dst" ]]; then
    error "Error: disk spec '$spec' invalid: dst '$dst' must be a directory if it exists"
  fi

  if [[ ! -f "$src" ]]; then
    
    if [[ -z "$size" ]]; then
      error "Error: disk spec '$spec' invalid: size must be specified if src '$src' does not exist"
    fi

    # Create directory for disk backing file, if needed.
    dir="$(busybox dirname "$src")"
    if ! [ -d "$dir" ]; then
      mkdir -p $(busybox dirname "$src")
    fi

    # Create disk backing file.
    busybox truncate -s "$size" "$src" >&2 || error "Error: disk spec '$spec' invalid: truncate on '$src' with size '$size' failed"

    # Create filesystem on disk backing file, populated with any pre-existing files from dst.
    [ -d "$RUNCVM_VM_MOUNTPOINT/$dst" ]|| mkdir -p "$RUNCVM_VM_MOUNTPOINT/$dst" >&2
    mke2fs -q -F -t "$fs" -d "$RUNCVM_VM_MOUNTPOINT/$dst" "$src" >&2 || error "Error: disk spec '$spec' invalid: mke2fs on '$src' with fs '$fs' failed"
  fi

  # Create the mountpoint, if we haven't already/it didn't already exist.
  [ -d "$RUNCVM_VM_MOUNTPOINT/$dst" ]|| mkdir -p "$RUNCVM_VM_MOUNTPOINT/$dst" >&2

  # Obtain a UUID for the filesystem and add to fstab.
  read -r UUID <<< $(blkid -o value "$src")
  echo "UUID=$UUID $dst $fs defaults,noatime 0 0" >>/.runcvm/fstab

  # Add disk to QEMU command line.

  if [ "${RUNCVM_CPUS:-1}" -gt 8 ]; then
    queues=8
  else
    queues="${RUNCVM_CPUS:-1}"
  fi

  DISKS+=(
    # Create a block backend (no implicit frontend device attached)
    # -drive id=drv$id       : give this backend a unique identifier
    # file=$src              : source image file
    # if=none                : do not attach directly, will be bound later with -device
    # format=raw             : treat the file as raw block device
    # media=disk             : marks this as a disk, not a CD-ROM
    # cache=none             : use O_DIRECT, bypass host page cache, rely on guest flushes for durability
    # aio=io_uring           : use Linux io_uring for asynchronous I/O (lower overhead than libaio)
    # discard=unmap          : propagate guest TRIM/UNMAP to host (space reclamation)
    # detect-zeroes=unmap    : convert guest zero writes into UNMAPs for efficiency
    -drive id=drv$id,file=$src,if=none,format=raw,media=disk,cache=none,aio=io_uring,discard=unmap,detect-zeroes=unmap

    # Create a dedicated I/O thread (dataplane) to offload block I/O
    # -object iothread,id=ioth$id : defines an iothread with a unique identifier
    -object iothread,id=ioth$id

    # Attach a virtio-blk PCI frontend to the backend and iothread
    # -device virtio-blk-pci     : create a virtio block device on PCI bus
    # drive=drv$id               : connect to backend created above
    # iothread=ioth$id           : process requests in dedicated I/O thread
    # queue-size=1024            : set depth of each virtqueue
    # num-queues=4               : create 4 queues, allowing parallel I/O across vCPUs
    -device virtio-blk-pci,drive=drv$id,iothread=ioth$id,queue-size=1024,num-queues=$queues
  )
}

# Argument e.g. /disk1,/home,ext4,5G;/disk2,/var,ext4,1G
do_disks() {
  local IFS=';'
  local disk
  local id=0
  for disk in $1
  do
    do_disk "$disk" "$id"
    id=$((id+1))
  done
}

# Original Docker networking - uses tap devices attached to bridges
do_networks_docker() {
  local id=0 ifpath if mac vhost
  local DOCKER_IF DOCKER_IF_MAC DOCKER_IF_MTU DOCKER_IF_IP DOCKER_IF_IP_NETPREFIX DOCKER_IF_IP_GW

  for ifpath in /.runcvm/network/devices/*
  do
    if=$(busybox basename "$ifpath")

    [ "$if" = "default" ] && continue

    load_network "$if"

    mac=$(busybox sed -r 's/^..:..:../52:54:00/' <<<$DOCKER_IF_MAC)

    if [ "$RUNCVM_QEMU_NET_VHOST" = "1" ]; then
      vhost="on"
    else
      vhost="off"
    fi

    IFACES+=(
        -netdev tap,id=qemu$id,ifname=tap-$DOCKER_IF,script=$QEMU_IFUP,downscript=$QEMU_IFDOWN,vhost=$vhost
        -device virtio-net-pci,netdev=qemu$id,mac=$mac,rombar=$id
    )

    id=$((id+1))
  done
}

# Kubernetes networking - uses SLIRP (user-mode) networking
# The pod already has network configured by CNI, we use user-mode networking
# to give the VM access through the existing network namespace
do_networks_kubernetes() {
  local id=0 ifpath if mac
  local DOCKER_IF DOCKER_IF_MAC DOCKER_IF_MTU DOCKER_IF_IP DOCKER_IF_IP_NETPREFIX DOCKER_IF_IP_GW
  local net_opts dns_servers

  echo "RunCVM: Using SLIRP networking for Kubernetes mode" >&2

  # Get DNS servers from resolv.conf
  dns_servers=""
  if [ -f /etc/resolv.conf ]; then
    while read -r line; do
      case "$line" in
        nameserver*)
          ns=$(echo "$line" | busybox awk '{print $2}')
          if [ -n "$ns" ] && [ "$ns" != "127.0.0.11" ]; then
            if [ -z "$dns_servers" ]; then
              dns_servers="$ns"
            else
              dns_servers="$dns_servers,$ns"
            fi
          fi
          ;;
      esac
    done < /etc/resolv.conf
  fi

  # Default to cluster DNS if nothing found
  if [ -z "$dns_servers" ]; then
    dns_servers="10.43.0.10"  # Default k3s/k8s CoreDNS
  fi

  for ifpath in /.runcvm/network/devices/*
  do
    if=$(busybox basename "$ifpath")

    [ "$if" = "default" ] && continue

    load_network "$if"

    mac=$(busybox sed -r 's/^..:..:../52:54:00/' <<<$DOCKER_IF_MAC)

    # Build SLIRP network options
    # - Use the pod's IP as the guest IP (so services can reach it)
    # - Forward DNS to cluster DNS
    # - Restrict to local network access
    
    # For the first (default) interface, set up full networking
    if [ "$if" = "$(busybox basename $(busybox readlink -f /.runcvm/network/devices/default 2>/dev/null || echo eth0))" ]; then
      # Calculate network and host portion for SLIRP
      # SLIRP will NAT the VM's traffic through the container's network namespace
      
      net_opts="user,id=qemu$id"
      net_opts="$net_opts,net=10.0.2.0/24"           # SLIRP internal network
      net_opts="$net_opts,host=10.0.2.2"             # Gateway (host from VM's perspective)
      net_opts="$net_opts,dns=10.0.2.3"              # DNS server in SLIRP network
      net_opts="$net_opts,dhcpstart=10.0.2.15"       # DHCP range start
      net_opts="$net_opts,hostfwd=tcp::2222-:22"     # Forward SSH for docker exec
      net_opts="$net_opts,hostname=$(busybox hostname)"
      
      # Add DNS forwarding - SLIRP will forward DNS queries to these servers
      # The VM will use 10.0.2.3, which SLIRP forwards to the real DNS
      
      IFACES+=(
        -netdev "$net_opts"
        -device virtio-net-pci,netdev=qemu$id,mac=$mac,rombar=$id
      )
    else
      # Additional interfaces - simpler setup
      net_opts="user,id=qemu$id,net=10.0.$((id+3)).0/24"
      
      IFACES+=(
        -netdev "$net_opts"
        -device virtio-net-pci,netdev=qemu$id,mac=$mac,rombar=$id
      )
    fi

    id=$((id+1))
  done

  # If no interfaces were configured, create a default one
  if [ $id -eq 0 ]; then
    echo "RunCVM: No network interfaces found, creating default SLIRP interface" >&2
    
    net_opts="user,id=qemu0"
    net_opts="$net_opts,net=10.0.2.0/24"
    net_opts="$net_opts,host=10.0.2.2"
    net_opts="$net_opts,dns=10.0.2.3"
    net_opts="$net_opts,dhcpstart=10.0.2.15"
    net_opts="$net_opts,hostfwd=tcp::2222-:22"
    net_opts="$net_opts,hostname=$(busybox hostname)"
    
    IFACES+=(
      -netdev "$net_opts"
      -device virtio-net-pci,netdev=qemu0,mac=52:54:00:12:34:56,rombar=0
    )
  fi
}

# Choose networking mode based on environment
do_networks() {
  if [ -f "/.runcvm/network/kubernetes_mode" ] || [ "$RUNCVM_KUBERNETES_MODE" = "1" ]; then
    do_networks_kubernetes
  else
    do_networks_docker
  fi
}

DISKS=()
if [ -n "$RUNCVM_DISKS" ]; then
  do_disks "$RUNCVM_DISKS"
fi

IFACES=()
do_networks

if [ -n "$RUNCVM_TMPFS" ]; then
  echo "$RUNCVM_TMPFS" >>/.runcvm/fstab
fi

if [[ -z "$RUNCVM_CPUS" || "$RUNCVM_CPUS" -le 0 ]]; then
  RUNCVM_CPUS=$(busybox nproc)
fi

# TODO:
# - Consider using '-device pvpanic'

# Auto-detect architecture if not set
if [ -z "$RUNCVM_QEMU_ARCH" ]; then
  case "$(cat /proc/sys/kernel/arch 2>/dev/null || busybox uname -m 2>/dev/null || echo aarch64)" in
    aarch64|arm64) RUNCVM_QEMU_ARCH="arm64" ;;
    x86_64) RUNCVM_QEMU_ARCH="x86_64" ;;
    *) RUNCVM_QEMU_ARCH="x86_64" ;;
  esac
fi

if [ "$RUNCVM_QEMU_ARCH" = "arm64" ]; then
  CMD="$(which qemu-system-aarch64)"
  MACHINE+=(-cpu max -machine virt,gic-version=max,usb=off)
  # Note: driftfix=slew is not supported on ARM virt machine, removed
  MACHINE+=(-rtc base=utc)
else
  CMD="$(which qemu-system-aarch64)"
  MACHINE+=(-enable-kvm)
  MACHINE+=(-cpu host,pmu=off)

  # QEMU machine configuration:
  # - q35: modern Intel chipset with PCIe and UEFI support
  # - accel=kvm: enable KVM for near-native performance (requires hardware virtualization)
  # - usb=on: enable USB support (e.g., for peripherals or boot devices)
  # - sata=off: disable SATA controller if using virtio/NVMe instead
  # - vmport=off: disable VMware backdoor port for improved security/stealth
  # - smm=on: enable System Management Mode (required for UEFI Secure Boot)
  # - dump-guest-core=off: disable saving core dumps on guest crash (saves disk space)
  # - hpet=off: disable High Precision Event Timer to avoid timing issues in some guests
  # - kernel_irqchip=on: enable kernel-based interrupt controller for better performance

  # - usb=on: enable USB support when requested
  if [ "$RUNCVM_QEMU_USB" = "on" ]; then
    USB="on"
  else
    USB="off"
  fi

  MACHINE+=(-machine q35,accel=kvm,sata=off,vmport=off,smm=on,dump-guest-core=off,hpet=off,usb=$USB,kernel_irqchip=on)

  # Enable the QEMU debug exit device for debugging purposes.
  # This device allows the guest to trigger a debug exit, which can be useful for debugging
  MACHINE+=(-device isa-debug-exit)

  # Set the virtual hardware clock to UTC time and enable drift correction using slewing.
  # This ensures the VM clock starts in UTC and gradually adjusts for time drift,
  # providing smooth and accurate timekeeping without sudden changes.
  MACHINE+=(-rtc base=utc,driftfix=slew)
fi

# RUNCVM_QEMU_DISPLAY -- the display frontend: none, curses
# RUNCVM_QEMU_VGA -- the backend VGA video device: none, std, virtio, cirrus
# RUNCVM_QEMU_VNC_DISPLAY -- the VNC display number (where enabled): 0, 1, ...

CONSOLE=()
DISPLAY=()

case "${RUNCVM_DISPLAY_MODE:-headless}" in
  headless) # virtconsole / hvc0
    RUNCVM_QEMU_DISPLAY="${RUNCVM_QEMU_DISPLAY:-none}"
    RUNCVM_QEMU_VGA="${RUNCVM_QEMU_VGA:-none}"

    # Creates a stdio backend connected to the virtual console.
    # Use with /dev/hvc0
    CONSOLE+=(
      -chardev stdio,id=char0,mux=off,signal=off
      -device virtconsole,chardev=char0,id=console0
    )

    CONSOLE_DEV="hvc0"
    NOMODESET="1"
    ;;
  serial) # stdio + monitor / ttyS0
    RUNCVM_QEMU_DISPLAY="${RUNCVM_QEMU_DISPLAY:-none}"
    RUNCVM_QEMU_VGA="${RUNCVM_QEMU_VGA:-none}"

    # Creates a multiplexed stdio backend connected to the serial port (and the QEMU monitor).
    # Use with /dev/ttyS0
    CONSOLE+=(
      -chardev stdio,id=char0,mux=on,signal=off
      -serial chardev:char0 -mon chardev=char0
    )

    # Set monitor escape key to CTRL-T to reduce risk of conflict (as default, CTRL-A, is  commonly used)
    CONSOLE+=(-echr 20)
    CONSOLE_DEV="ttyS0"
    NOMODESET="1"
    ;;
  vnc) # VNC + tty0
    RUNCVM_QEMU_DISPLAY="${RUNCVM_QEMU_DISPLAY:-none}"
    RUNCVM_QEMU_VGA="${RUNCVM_QEMU_VGA:-virtio}"
    RUNCVM_QEMU_VNC_DISPLAY="${RUNCVM_QEMU_VNC_DISPLAY:-0}"

    CONSOLE_DEV="tty0"
    NOMODESET="0"
    ;;
  *)
    error "Error: RUNCVM_DISPLAY_MODE '$RUNCVM_DISPLAY_MODE' invalid, must be one of: default|headless, serial, vnc" ;;
esac

# Save choice of console device
echo "$CONSOLE_DEV" >/.runcvm/console

# Generate QEMU display options
if [ -n "$RUNCVM_QEMU_DISPLAY" ]; then
  DISPLAY+=(-display $RUNCVM_QEMU_DISPLAY)
fi

if [ -n "$RUNCVM_QEMU_VGA" ]; then

  # i.e. virtio, std, none
  DISPLAY+=(-vga $RUNCVM_QEMU_VGA)

  if [ "$RUNCVM_QEMU_VGA" != "none" ]; then
    NOMODESET="0" # Disable nomodeset if using a graphical console
    DISPLAY+=(-device virtio-tablet-pci)
    DISPLAY+=(-audiodev none,id=snd0 -device intel-hda -device hda-output,audiodev=snd0)

    # For now, unless/until `-device virtio-gpu-pci` support is provided,
    # the VNC display is only relevant if VGA is not 'none'
    if [ -n "$RUNCVM_QEMU_VNC_DISPLAY" ]; then
      DISPLAY+=(-vnc :$RUNCVM_QEMU_VNC_DISPLAY,password=off)
    fi
  fi
fi

# Append nomodeset unless using a graphical console (i.e. not serial or virtconsole)
# PROBABLY ONLY NEEDED FOR `-display curses` (RUNCVM_QEMU_DISPLAY=curses) CASE
if [ "$NOMODESET" = "1" ]; then
  APPEND+=(nomodeset)
fi

if [ "$RUNCVM_QEMU_ARCH" != "arm64" ] && [ "$RUNCVM_BIOS_DEBUG" != "1" ]; then
  # Disable SeaBIOS serial console.
  # This -cfw_cfg path is modified from the SeaBIOS default (to avoid an otherwise-inevitable QEMU
  # warning being emitted) and so requires patched bios.bin file(s) (see Dockerfile)
  OPTS+=(-fw_cfg opt/org.seabios/etc/sercon-port,string=0)
fi

if [ "$RUNCVM_BIOS" = "EFI" ] || [ "$RUNCVM_QEMU_ARCH" = "arm64" ]; then
  # UEFI firmware - use AAVMF for ARM64, OVMF for x86
  if [ "$RUNCVM_QEMU_ARCH" = "arm64" ]; then
    OPTS+=(-bios $RUNCVM_GUEST/usr/share/AAVMF/QEMU_EFI.fd)
  else
    OPTS+=(-bios $RUNCVM_GUEST/usr/share/OVMF/OVMF.fd)
  fi
fi

MEM_BACKEND=(-numa node,memdev=mem)
if [ "$RUNCVM_HUGETLB" != "1" ]; then
  # Tests suggests prealloc=on slows down mem-path=/dev/shm
  MEM_PATH="/dev/shm" 
  MEM_BACKEND+=(-object memory-backend-file,id=mem,size=$RUNCVM_MEM_SIZE,mem-path=$MEM_PATH,share=on,prealloc=${RUNCVM_QEMU_MEM_PREALLOC:-off})
else
  # Fastest performance: +15% CPU/net intensive; 3.5x disk intensive.
  MEM_BACKEND+=(-object memory-backend-memfd,id=mem,size=$RUNCVM_MEM_SIZE,share=on,prealloc=${RUNCVM_QEMU_MEM_PREALLOC:-off},hugetlb=on)
fi

# 16-64 works well and is more performant than 1024 in some scenarios.
# For now, stick with original figure.
VIRTIOFS_QUEUE_SIZE=1024
VIRTIOFS+=(
  -chardev socket,id=virtiofs,path=$QEMU_VIRTIOFSD_SOCKET
  -device vhost-user-fs-pci,queue-size=$VIRTIOFS_QUEUE_SIZE,chardev=virtiofs,tag=runcvmfs,ats=off
)

# Experimental: Enable to specify a dedicated PCI bridge
# OPTS+=(-device pci-bridge,bus=pcie.0,id=pci-bridge-0,chassis_nr=1,shpc=off,addr=2,io-reserve=4k,mem-reserve=1m,pref64-reserve=1m)

# Experimental: Enable for a SCSI bus
# OPTS+=(-device virtio-scsi-pci,id=scsi0,disable-modern=true)

# Disable IPv6, which is currently unsupported, at kernel boot time
APPEND+=(ipv6.disable=1 panic=-1)

# Disable unneeded functionality
APPEND+=(scsi_mod.scan=none tsc=reliable no_timer_check rcupdate.rcu_expedited=1 i8042.direct=1 i8042.dumbkbd=1 i8042.nopnp=1 i8042.noaux=1 noreplace-smp reboot=k cryptomgr.notests pci=lastbus=0 selinux=0)

# Enable systemd startup logging by default:
# - Only effective when --env=RUNCVM_KERNEL_DEBUG=1
# - Override this by launching with --env='RUNCVM_KERNEL_APPEND=systemd.show_status=0'
APPEND+=(systemd.show_status=1)

if [ "$RUNCVM_KERNEL_DEBUG" = "1" ]; then
  APPEND+=(console=$CONSOLE_DEV)
else
  APPEND+=(quiet)
fi

ARGS=(
  -no-user-config
  -nodefaults
  -no-reboot

  -action panic=none
  -action reboot=shutdown

  "${MACHINE[@]}"
  "${DISPLAY[@]}"
  "${OPTS[@]}"

  # N.B. There is a counterintuitive relationship between cpus and memory, and performance:
  # - more cpus needs more memory to maintain the same virtiofs disk I/O performance.
  -m "$RUNCVM_MEM_SIZE"
  -smp $RUNCVM_CPUS,cores=1,threads=1,sockets=$RUNCVM_CPUS,maxcpus=$RUNCVM_CPUS

  # Creates a virtio-serial bus on the PCI bus; this is used for the guest agent and virtiofs
  -device virtio-serial-pci,id=serial0 

  # Creates an RNG on the PCI bus
  -object rng-random,id=rng0,filename=/dev/urandom -device virtio-rng-pci,rng=rng0

  # Memory backend
  "${MEM_BACKEND[@]}"

  # virtiofs socket and interface
  "${VIRTIOFS[@]}"

  # Configure host/container tap device with PXE roms disabled
  "${IFACES[@]}"
  "${DISKS[@]}"

  # Configure console
  "${CONSOLE[@]}"

  # Support for guest agent
  -chardev socket,id=qemuguest0,path=$QEMU_GUEST_AGENT,server=on,wait=off
  -device virtserialport,chardev=qemuguest0,name=org.qemu.guest_agent.0

  # Creates a unix socket for the QEMU monitor
  -monitor unix:$QEMU_MONITOR_SOCKET,server,nowait

  # Kernel and initrd and kernel cmdline
  -kernel $RUNCVM_KERNEL_PATH
  -initrd $RUNCVM_KERNEL_INITRAMFS_PATH
  -L $RUNCVM_GUEST/usr/share/qemu
  -append "$RUNCVM_KERNEL_ROOT $INIT rw ${APPEND[*]} $RUNCVM_KERNEL_APPEND"
)

if [[ "$RUNCVM_BREAK" =~ preqemu ]]; then echo "Preparing to run: '$CMD' ${ARGS[@]@Q}"; bash; fi

# Always show what we're about to run
echo "Preparing to run: '$CMD' ${ARGS[@]@Q}" >&2

# Debug: Check critical paths
echo "RunCVM: Checking critical paths before QEMU..." >&2
echo "RunCVM: /dev/shm: $(ls -la /dev/shm 2>&1 | head -3)" >&2
echo "RunCVM: virtiofs socket: $(ls -la /run/.virtiofs.sock 2>&1)" >&2
echo "RunCVM: VM mountpoint: $(ls -la /vm 2>&1 | head -3)" >&2
echo "RunCVM: Memory available: $(free -m 2>&1 | head -2)" >&2

if [ "$RUNCVM_QEMU_DEBUG" = "1" ]; then
  "$CMD" "${ARGS[@]}" || echo "QEMU exited with code: $?" >&2
  echo "QEMU memory cgroup events:" >&2
  cat /sys/fs/cgroup/memory.events >&2
else
exec "$CMD" "${ARGS[@]}"
fi
