#!/.runcvm/guest/bin/bash

# RunCVM Firecracker Launcher
# Launches a Firecracker microVM using config file mode (for console output)
#
# STRATEGY: Copy container files to /dev/shm, then create rootfs on overlay
# This avoids the self-copy problem where mke2fs would copy the rootfs into itself

set -o errexit -o pipefail

# Load original environment
. /.runcvm/config

# Load defaults
. $RUNCVM_GUEST/scripts/runcvm-ctr-defaults && unset PATH

FIRECRACKER_BIN="$RUNCVM_GUEST/sbin/firecracker"
FIRECRACKER_CONFIG="/run/.firecracker-config.json"
NFS_CONFIG="/.runcvm/nfs-mounts"
NFS_PORT_MIN=1000
NFS_PORT_MAX=1050

# ============================================================
# LOGGING SYSTEM
# Severity levels: DEBUG, INFO, ERROR, OFF
# Control via: RUNCVM_LOG_LEVEL environment variable
# ============================================================

# Default log level (can be overridden by environment)
RUNCVM_LOG_LEVEL="${RUNCVM_LOG_LEVEL:-OFF}"

# Log severity levels (numeric for comparison)
declare -A LOG_LEVELS=(
  [DEBUG]=0
  [INFO]=1
  [LOG]=1      # Alias for INFO
  [ERROR]=2
  [OFF]=999
)

# Get numeric level for current log level (default to OFF=999)
CURRENT_LOG_LEVEL="${LOG_LEVELS[$RUNCVM_LOG_LEVEL]:-999}"

# Core logging function
_log() {
  local severity="$1"
  shift
  local message="$*"
  local severity_level="${LOG_LEVELS[$severity]:-1}"
  
  # Only log if severity meets threshold
  if [ "$severity_level" -ge "$CURRENT_LOG_LEVEL" ]; then
    echo "[$(busybox date '+%Y-%m-%d %H:%M:%S')] [RunCVM-FC] [$severity] $message" >&2
  fi
}

# Convenience functions for different severity levels
log_debug() {
  _log DEBUG "$@"
}

log_info() {
  _log INFO "$@"
}

log() {
  # Default log function - maps to INFO for backward compatibility
  _log INFO "$@"
}

log_error() {
  _log ERROR "$@"
}

error() {
  # ALWAYS log errors to stderr, regardless of log level
  echo "[$(busybox date '+%Y-%m-%d %H:%M:%S')] [RunCVM-FC] [ERROR] $@" >&2
  exit 1
}

create_volumes_from_vm_mounts() {
  local volumes_file="/.runcvm/volumes"
  local vm_mountpoint="${RUNCVM_VM_MOUNTPOINT:-/vm}"
  
  log "Creating volumes file from mounted directories..."
  
  > "$volumes_file"
  
  # Look at directories under /vm that are mountpoints
  for dir in "$vm_mountpoint"/*; do
    [ -d "$dir" ] || continue
    
    # Skip system directories
    case "$dir" in
      */etc|*/proc|*/sys|*/dev|*/run|*/.runcvm) continue ;;
    esac
    
    # Check if it's a mountpoint (different device than parent)
    if busybox mountpoint -q "$dir" 2>/dev/null; then
      local container_path="${dir#$vm_mountpoint}"
      # Get the source from findmnt or /proc/mounts
      local source=$(busybox awk -v mp="$dir" '$2 == mp {print $1; exit}' /proc/mounts)
      
      # If source is a device, try to find the actual bind source
      if [ -n "$source" ]; then
        log "  Found mountpoint: $dir (source: $source)"
        echo "${dir}:${container_path}:rw" >> "$volumes_file"
      fi
    fi
  done
  
  if [ -s "$volumes_file" ]; then
    log "Volumes file created with $(busybox wc -l < "$volumes_file") entries"
    busybox cat "$volumes_file" | while read line; do log "  $line"; done
  else
    log "No volume mountpoints detected"
  fi
}

setup_nfs_volumes() {
  # NFS volume sync using HOST-SIDE unfsd
  # Architecture:
  #   Host: runcvm-runtime starts unfsd daemon and sets RUNCVM_NFS_VOLUMES env var
  #   Container: reads env var and writes NFS config for guest VM
  #   Guest: mounts via NFS client (kernel built-in)
  #
  # RUNCVM_NFS_VOLUMES format: src:dst:port|src2:dst2:port2|...
  
  local nfs_config="$NFS_CONFIG"
  
  # DEBUG: Show what we have (only if log level is DEBUG)
  if [ "$RUNCVM_LOG_LEVEL" = "DEBUG" ]; then
    echo "[DEBUG] setup_nfs_volumes checking env var" >&2
    echo "[DEBUG] RUNCVM_NFS_VOLUMES=${RUNCVM_NFS_VOLUMES:-<not set>}" >&2
  fi
  
  # Read from environment variable (set by runcvm-runtime)
  if [ -n "$RUNCVM_NFS_VOLUMES" ]; then
    > "$nfs_config"
    
    log INFO "Setting up NFS volumes from RUNCVM_NFS_VOLUMES env"
    
    # Parse pipe-separated entries
    echo "$RUNCVM_NFS_VOLUMES" | busybox tr '|' '\n' | while IFS=: read -r src dst port; do
      if [ -z "$src" ]; then continue; fi
      if [ -z "$port" ]; then port="2049"; fi
      
      log INFO "  Volume: $src -> $dst (port $port)"
      if [ "$RUNCVM_LOG_LEVEL" = "DEBUG" ]; then
         echo "[DEBUG] NFS config line: $src:$dst:$port" >&2
      fi
      
      # Write config for guest (format: src:dst:port)
      echo "$src:$dst:$port" >> "$nfs_config"
    done
    
    log INFO "  NFS config written to $nfs_config"
    if [ "$RUNCVM_LOG_LEVEL" = "DEBUG" ]; then
      echo "[DEBUG] Final nfs_config: $(busybox cat $nfs_config)" >&2
    fi
    
  else
    log INFO "RUNCVM_NFS_VOLUMES not set, skipping NFS volume setup"
  fi
}

cleanup_nfs_volumes() {
  # NFS cleanup is handled by runcvm-runtime on container stop
  # Nothing to do here
  :
}

trap cleanup_nfs_volumes EXIT SIGTERM SIGINT

# Create rootfs image from a source directory
create_rootfs_from_dir() {
  local source_dir="$1"
  local image_path="$2"
  local size_mb="$3"
  
  log "Creating ext4 rootfs: $image_path (${size_mb}MB) from $source_dir"
  
  # Create sparse file
  if ! busybox truncate -s "${size_mb}M" "$image_path"; then
    error "Failed to create sparse file"
  fi
  
  # Create ext4 filesystem populated with source directory contents
  if [ "$RUNCVM_LOG_LEVEL" = "DEBUG" ]; then
    # Show mke2fs output in debug mode
    if ! mke2fs -F -t ext4 -E root_owner=0:0 -d "$source_dir" "$image_path" 2>&1; then
      log "mke2fs failed"
      busybox rm -f "$image_path"
      return 1
    fi
  else
    # Silent mode - redirect to /dev/null
    if ! mke2fs -F -t ext4 -E root_owner=0:0 -d "$source_dir" "$image_path" >/dev/null 2>&1; then
      log_error "mke2fs failed"
      busybox rm -f "$image_path"
      return 1
    fi
  fi
  log "Rootfs created successfully"
  return 0
}

# Main function
main() {
  log "Starting Firecracker microVM launcher..."
  # ==========================================================================
  # STEP 0: Extract volumes and start 9P servers BEFORE creating rootfs
  # ==========================================================================
  
  # Create volumes file if runtime didn't create it
  # NOTE: Runtime creates at $ROOT/.runcvm/volumes which is accessible via /vm/.runcvm/volumes
  # because /.runcvm is a tmpfs that hides $ROOT/.runcvm
  if [ ! -f "/vm/.runcvm/volumes" ]; then
    create_volumes_from_vm_mounts
  fi

  setup_nfs_volumes

  log_debug "NFS_CONFIG=$NFS_CONFIG"
  if [ -f "$NFS_CONFIG" ]; then
    log_debug "Contents of $NFS_CONFIG:"
    busybox cat "$NFS_CONFIG" 2>&1 | while read line; do log_debug "  $line"; done
  else
    log_debug "NFS_CONFIG file does not exist (no volumes)"
  fi
  
  # Check for Firecracker binary
  if [ ! -x "$FIRECRACKER_BIN" ]; then
    error "Firecracker binary not found: $FIRECRACKER_BIN"
  fi
  
  # ==========================================================================
  # STRATEGY: Copy container files to /dev/shm, then create rootfs on overlay
  # ==========================================================================
  # Problem: /vm is a bind mount of /, so any file at /x is also at /vm/x
  #          This causes mke2fs -d /vm to copy the rootfs into itself
  #
  # Solution: 
  #   1. Copy /vm contents to /dev/shm/rootfs-staging (separate tmpfs)
  #   2. Create rootfs at /rootfs.ext4 (on overlay, has disk space)
  #   3. mke2fs reads from /dev/shm, writes to /rootfs.ext4
  #   4. No self-copy because source (/dev/shm) != destination parent (/)
  # ==========================================================================
  
  local staging_dir="/dev/shm/rootfs-staging"
  local rootfs_path="/rootfs.ext4"
  
  # Calculate source size (excluding .runcvm which has RunCVM binaries)
  log "Calculating container size (excluding .runcvm)..."
  
  local source_size=$(busybox du -sm --exclude='.runcvm' "$RUNCVM_VM_MOUNTPOINT" 2>/dev/null | busybox cut -f1)
  [ -z "$source_size" ] && source_size=100
  
  log "Container filesystem size: ${source_size}MB"
  
  # Check if /dev/shm has enough space for staging
  local shm_avail=$(busybox df -m /dev/shm 2>/dev/null | busybox awk 'NR==2 {print $4}')
  local shm_needed=$(( source_size + 50 ))  # Add 50MB buffer
  
  log "/dev/shm available: ${shm_avail}MB, need for staging: ${shm_needed}MB"
  
  if [ "$shm_avail" -lt "$shm_needed" ]; then
    log ""
    log "============================================================"
    log "ERROR: /dev/shm too small for staging"
    log "============================================================"
    log ""
    log "Container size: ${source_size}MB"
    log "/dev/shm available: ${shm_avail}MB"
    log "/dev/shm is sized by the -m (memory) flag"
    log ""
    log "SOLUTION: Increase VM memory to at least ${shm_needed}m"
    log ""
    log "  docker run --runtime=runcvm -m ${shm_needed}m \\"
    log "    -e RUNCVM_HYPERVISOR=firecracker nginx"
    log ""
    log "============================================================"
    error "Insufficient /dev/shm for staging (need ${shm_needed}MB, have ${shm_avail}MB)"
  fi
  
  # Step 1: Copy container files to staging area
  log "Step 1/3: Copying container files to staging area..."
  log "  Source: $RUNCVM_VM_MOUNTPOINT (excluding .runcvm)"
  log "  Destination: $staging_dir"
  log "  This may take 30-60 seconds for large images..."
  
  busybox rm -rf "$staging_dir"
  busybox mkdir -p "$staging_dir"
  
  # Use tar to copy, excluding .runcvm directory
  local copy_start=$(busybox date +%s)
  
  if ! (cd "$RUNCVM_VM_MOUNTPOINT" && busybox tar -cf - --exclude='.runcvm' . 2>/dev/null | busybox tar -xf - -C "$staging_dir" 2>/dev/null); then
    error "Failed to copy container files to staging"
  fi
  
  local copy_end=$(busybox date +%s)
  local copy_time=$(( copy_end - copy_start ))
  
  local staged_size=$(busybox du -sm "$staging_dir" 2>/dev/null | busybox cut -f1)
  log "  Staging complete: ${staged_size}MB copied in ${copy_time}s"
  
  # Step 1b: Create init script in staging area
  # This init script will be /init in the rootfs and runs the container's entrypoint
  log "  Creating init script..."
  
  # Read the original entrypoint from runcvm config
  local entrypoint_file="/.runcvm/entrypoint"
  local init_script="${staging_dir}/init"
  
  # Create a minimal init script
  cat > "$init_script" << 'INITEOF'
#!/bin/sh
# Firecracker minimal init for RunCVM
# This runs as PID 1 inside the Firecracker VM

# ============================================================
# LOGGING SYSTEM (sh-compatible)
# ============================================================

export PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin

# Try to load RUNCVM_LOG_LEVEL from config if it exists
if [ -f /.runcvm/config ]; then
  # Extract RUNCVM_LOG_LEVEL from config (format: declare -x RUNCVM_LOG_LEVEL="VALUE")
  RUNCVM_LOG_LEVEL=$(grep '^declare -x RUNCVM_LOG_LEVEL=' /.runcvm/config 2>/dev/null | sed 's/^declare -x RUNCVM_LOG_LEVEL="\(.*\)"$/\1/' | head -1)
fi

# Default to OFF if not set (matches host default for silent operation)
# NOTE: DSR terminal issue is fixed by having log output during boot; 
# users who want silent logs can set RUNCVM_LOG_LEVEL=OFF
RUNCVM_LOG_LEVEL="${RUNCVM_LOG_LEVEL:-OFF}"

# Logging function (simplified for sh)
# Logging function (simplified for sh)
log() {
  local severity="$1"
  shift
  local message="$*"
  
  # Always show ERRORs, otherwise check log level
  if [ "$severity" = "ERROR" ] || [ "$RUNCVM_LOG_LEVEL" = "DEBUG" ]; then
    echo "[$(date '+%Y-%m-%d %H:%M:%S')] [RunCVM-FC-Init] [$severity] $message"
    return
  fi

  case "$RUNCVM_LOG_LEVEL" in
    INFO|LOG)
      case "$severity" in
        INFO) echo "[$(date '+%Y-%m-%d %H:%M:%S')] [RunCVM-FC-Init] [$severity] $message" ;;
      esac
      ;;
  esac
}

# Helper function to redirect output based on log level
# Usage: some_command $(output_redirect)
output_redirect() {
  if [ "$RUNCVM_LOG_LEVEL" = "DEBUG" ]; then
    echo ""  # No redirection
  else
    echo ">/dev/null 2>&1"
  fi
}

# Simpler helper - returns 0 if debug
is_debug() {
  [ "$RUNCVM_LOG_LEVEL" = "DEBUG" ]
}


log INFO "Starting..."

export PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin

# DEBUG: Check environment
if is_debug; then
  log INFO "Environment check:"
  log INFO "  PATH: $PATH"
  log INFO "  busybox: $(which busybox 2>/dev/null || echo 'not found')"
  if [ -x /bin/busybox ]; then
     log INFO "  /bin/busybox exists"
     log INFO "  cttyhack in busybox: $(/bin/busybox --list | grep cttyhack || echo 'no')"
  else
     log INFO "  /bin/busybox missing"
  fi
fi

# Mount essential filesystems
mount -t proc proc /proc 2>/dev/null || true
mount -t sysfs sys /sys 2>/dev/null || true
mount -t devtmpfs dev /dev 2>/dev/null || true

# Create essential device nodes if devtmpfs failed
if [ ! -c /dev/null ]; then
  mknod -m 666 /dev/null c 1 3
  mknod -m 666 /dev/zero c 1 5
  mknod -m 666 /dev/random c 1 8
  mknod -m 666 /dev/urandom c 1 9
  mknod -m 666 /dev/tty c 5 0
  mknod -m 620 /dev/console c 5 1
  mknod -m 666 /dev/ptmx c 5 2
fi

# Mount pts for proper terminal support
mkdir -p /dev/pts /dev/shm
mount -t devpts devpts /dev/pts 2>/dev/null || true
mount -t tmpfs tmpfs /dev/shm 2>/dev/null || true

# Create tmpfs for /run and /tmp
mkdir -p /run /tmp
mount -t tmpfs tmpfs /run 2>/dev/null || true
mount -t tmpfs tmpfs /tmp 2>/dev/null || true

# Create symlinks
# Create symlinks
ln -sf /proc/self/fd /dev/fd 2>/dev/null || true
ln -sf /proc/self/fd/0 /dev/stdin 2>/dev/null || true
ln -sf /proc/self/fd/1 /dev/stdout 2>/dev/null || true
ln -sf /proc/self/fd/2 /dev/stderr 2>/dev/null || true

# Force all output to console (now that devices exist)
exec >/dev/console 2>&1

# Setup hostname
[ -f /etc/hostname ] && hostname -F /etc/hostname 2>/dev/null || true

# Setup networking
log INFO "========== NETWORK SETUP START =========="

# Setup RunCVM tools if available
# These tools use BundELF and need the dynamic linker from the same directory
# IMPORTANT: Path must match the original /.runcvm/guest/ because of relative RPATH
RUNCVM_GUEST="/.runcvm/guest"
if [ -d "$RUNCVM_GUEST/lib" ]; then
  # The dynamic linker is at /.runcvm/guest/lib/ld
  RUNCVM_LD="$RUNCVM_GUEST/lib/ld"
  if [ -x "$RUNCVM_LD" ]; then
    log INFO "Found RunCVM tools at $RUNCVM_GUEST"
    log INFO "Checking tools structure:"
    log DEBUG "lib/ld: $(ls -la $RUNCVM_GUEST/lib/ld 2>&1)"
    log DEBUG "bin contents: $(ls -la $RUNCVM_GUEST/bin/ 2>&1 | head -10)"
    log DEBUG "bin/ip: $(ls -la $RUNCVM_GUEST/bin/ip 2>&1)"
    log DEBUG "bin/busybox: $(ls -la $RUNCVM_GUEST/bin/busybox 2>&1)"
    
    # Test the dynamic linker directly
    log INFO "Testing dynamic linker..."
    log DEBUG "Test 1 - ld exists: $(test -x $RUNCVM_LD && echo yes || echo no)"
    log DEBUG "Test 2 - busybox via ld: $($RUNCVM_LD $RUNCVM_GUEST/bin/busybox echo 'works' 2>&1)"
    
    # ip is likely a symlink to busybox, so we need to call busybox ip
    # Create wrapper functions that use the dynamic linker with busybox
    runcvm_ip() { "$RUNCVM_LD" "$RUNCVM_GUEST/bin/busybox" ip "$@"; }
    runcvm_busybox() { "$RUNCVM_LD" "$RUNCVM_GUEST/bin/busybox" "$@"; }
    HAVE_RUNCVM_TOOLS=1
  else
    log INFO "Dynamic linker not found at $RUNCVM_LD"
  fi
else
  log INFO "RunCVM tools not found at $RUNCVM_GUEST"
fi

# Bring up loopback
if [ "$HAVE_RUNCVM_TOOLS" = "1" ]; then
  runcvm_ip link set lo up 2>/dev/null || true
else
  ip link set lo up 2>/dev/null || ifconfig lo up 2>/dev/null || true
fi

# First, check what kernel modules are loaded for networking
log INFO "Checking for virtio_net module..."
if is_debug && [ -f /proc/modules ]; then
  grep -i virtio /proc/modules 2>/dev/null || echo "  No virtio modules loaded"
fi

# Check /sys/class/net to see what the kernel sees
log INFO "Kernel network interfaces in /sys/class/net:"
if is_debug; then
  ls -la /sys/class/net/ 2>/dev/null || echo "  Cannot list /sys/class/net"
fi

# Check dmesg for network-related messages
log INFO "Recent dmesg network messages:"
if is_debug; then
  dmesg 2>/dev/null | grep -iE "(eth|net|virtio)" | tail -10 || echo "  Cannot read dmesg"
fi

# Configure all network interfaces from config files or DHCP
log INFO "Configuring network..."

# We need to find all eth* interfaces
# Since we might not have 'ls' or 'find' behaving standardly, we look at sysfs
if is_debug; then
  ls -la /sys/class/net/
fi

# Iterate over eth interfaces found in sysfs
found_ifaces=0
for iface_path in /sys/class/net/eth*; do
  # Check if glob expansion failed
  [ -e "$iface_path" ] || continue
  
  IFACE=$(basename "$iface_path")
  found_ifaces=1
  log INFO "Configuring interface $IFACE..."

  # Look for config file
  CONFIG_FILE="/.runcvm-network-${IFACE}"
  
  # Backward compat: check legacy file for eth0 if new one missing
  if [ "$IFACE" = "eth0" ] && [ ! -f "$CONFIG_FILE" ] && [ -f "/.runcvm-network" ]; then
     CONFIG_FILE="/.runcvm-network"
  fi
  
  if [ -f "$CONFIG_FILE" ]; then
    log INFO "  Loading config from $CONFIG_FILE"
    if is_debug; then cat "$CONFIG_FILE"; fi
    
    # unset previous vars to be safe
    unset FC_IP FC_PREFIX FC_GW FC_MTU FC_MAC
    . "$CONFIG_FILE"
    
    if [ -n "$FC_IP" ] && [ -n "$FC_PREFIX" ]; then
       log INFO "  Setting IP $FC_IP/$FC_PREFIX (MTU: ${FC_MTU:-1500})"
       
       # Determine tool to use
       if [ "$HAVE_RUNCVM_TOOLS" = "1" ]; then
         IP_CMD="runcvm_ip"
       elif command -v ip >/dev/null 2>&1; then
         IP_CMD="ip"
       elif command -v ifconfig >/dev/null 2>&1; then
         IP_CMD=""
         USE_IFCONFIG=1
       else
         IP_CMD=""
       fi
       
       if [ -n "$IP_CMD" ]; then
          $IP_CMD link set "$IFACE" up 2>/dev/null || true
          [ -n "$FC_MTU" ] && $IP_CMD link set "$IFACE" mtu "$FC_MTU" 2>/dev/null || true
          $IP_CMD addr add "$FC_IP/$FC_PREFIX" dev "$IFACE"
          
          if [ -n "$FC_GW" ] && [ "$FC_GW" != "-" ]; then
             log INFO "  Adding default gateway $FC_GW"
             $IP_CMD route add default via "$FC_GW" dev "$IFACE"
             
             # If this is the default GW interface, add the bridge route for 9P too
             # (This is mostly relevant for the primary interface)
             $IP_CMD route add 169.254.1.1/32 dev "$IFACE" 2>/dev/null || true
          fi
          
       elif [ "$USE_IFCONFIG" = "1" ]; then
          # Basic ifconfig support
          case "$FC_PREFIX" in
            8)  FC_NETMASK="255.0.0.0" ;;
            16) FC_NETMASK="255.255.0.0" ;;
            24) FC_NETMASK="255.255.255.0" ;;
            *)  FC_NETMASK="255.255.255.0" ;;
          esac
          ifconfig "$IFACE" "$FC_IP" netmask "$FC_NETMASK" up
          if [ -n "$FC_GW" ] && [ "$FC_GW" != "-" ]; then
             route add default gw "$FC_GW" 2>/dev/null || true
          fi
       fi
    else
       log ERROR "  Config file found but missing IP/Prefix!"
    fi
  else
    # Fallback to DHCP if no config
    log INFO "  No config file, trying DHCP..."
    ip link set "$IFACE" up 2>/dev/null || ifconfig "$IFACE" up 2>/dev/null || true
    if command -v udhcpc >/dev/null 2>&1; then
      udhcpc -i "$IFACE" -n -q 2>/dev/null || true
    fi
  fi
done

if [ "$found_ifaces" = "0" ]; then
  log INFO "No ethernet interfaces found!"
fi

log INFO "========== NETWORK SETUP END =========="

# Setup DNS
if [ -f /.runcvm-resolv.conf ]; then
  cp /.runcvm-resolv.conf /etc/resolv.conf 2>/dev/null || true
fi

# ==========================================================================
# NFS VOLUME MOUNTS - Mount NFS shares from host unfsd
# ==========================================================================
mount_nfs_volumes() {
  local nfs_config="/.runcvm/nfs-mounts"
  
  # Get gateway IP from network config (this is the HOST from VM's perspective)
  # Get gateway IP from network config (this is the HOST from VM's perspective)
  local host_ip=""
  
  # Scan all network configs for a gateway
  # We prioritize eth0 if it exists
  if [ -f "/.runcvm-network-eth0" ]; then
     unset FC_GW
     . "/.runcvm-network-eth0"
     if [ -n "$FC_GW" ] && [ "$FC_GW" != "-" ]; then
       host_ip="$FC_GW"
     fi
  fi
  
  # If not found in eth0, try others
  if [ -z "$host_ip" ]; then
    for cfg in /.runcvm-network-*; do
      [ -f "$cfg" ] || continue
      unset FC_GW
      . "$cfg"
      if [ -n "$FC_GW" ] && [ "$FC_GW" != "-" ]; then
        host_ip="$FC_GW"
        break
      fi
    done
  fi
  
  # Fallback to default Docker gateway if still nothing
  if [ -z "$host_ip" ] || [ "$host_ip" = "-" ]; then
    log INFO "Warning: No gateway found in network configs, defaulting to 172.17.0.1"
    host_ip="172.17.0.1"
  fi
  
  log INFO "Checking for NFS volumes..."
  log INFO "  Host IP (for NFS): $host_ip"
  
  if [ ! -f "$nfs_config" ]; then
    log INFO "No nfs-mounts config found - volumes are static copies"
    return 0
  fi
  
  log INFO "NFS Transport: TCP over $host_ip"
  log INFO "Mount type: NFS (live, bidirectional)"
  
  # Config format: src:dst:port
  # Mount each volume via NFS
  while IFS=: read -r src_path dst nfs_port; do
    [ -z "$src_path" ] && continue
    [ -z "$dst" ] && continue
    
    log INFO "  Mounting $src_path -> $dst (port $nfs_port)..."
    mkdir -p "$dst"
    
    # Mount via NFS v3 with nolock (no separate lockd needed)
    mount -t nfs -o vers=3,nolock,tcp,port="$nfs_port",mountport="$((nfs_port + 1))" \
      "$host_ip:$src_path" "$dst" 2>&1 | sed 's/^/    /'
    
    if mount | grep -q "$dst"; then
      log INFO "  ✓ Successfully mounted $dst (NFS)"
    else
      log ERROR "  ✗ Failed to mount $dst via NFS"
      log INFO "    Falling back to static copy mode"
    fi
  done < "$nfs_config"
  
  log INFO "  NFS mounts complete"
}

log INFO "========== NFS VOLUME MOUNTS =========="
mount_nfs_volumes
log INFO "========== NFS VOLUME MOUNTS END =========="

# ========== DROPBEAR SSH SERVER FOR DOCKER EXEC ==========
# docker exec uses SSH to connect to the VM, so we need dropbear running
log INFO "========== DROPBEAR SSH SETUP =========="

SSHD_PORT=22222
DROPBEAR_DIR="/.runcvm/dropbear"

if [ "$HAVE_RUNCVM_TOOLS" = "1" ]; then
  mkdir -p "$DROPBEAR_DIR"
  
  # Check if we need to generate keys
  if [ ! -f "$DROPBEAR_DIR/key" ]; then
    log INFO "Generating dropbear SSH keys..."
    # Generate ed25519 key
    "$RUNCVM_LD" "$RUNCVM_GUEST/usr/bin/dropbearkey" -t ed25519 -f "$DROPBEAR_DIR/key" >/dev/null 2>&1
    
    # Extract public key for EPKA
    KEY_PUBLIC=$("$RUNCVM_LD" "$RUNCVM_GUEST/usr/bin/dropbearkey" -y -f "$DROPBEAR_DIR/key" 2>/dev/null | grep ^ssh | cut -d' ' -f2)
    
    # Create EPKA config
    cat > "$DROPBEAR_DIR/epka.json" << EPKAEOF
[{"user":"root","keytype":"ssh-ed25519","key":"$KEY_PUBLIC","options":"no-X11-forwarding","comments":""}]
EPKAEOF
    chmod 400 "$DROPBEAR_DIR/epka.json" "$DROPBEAR_DIR/key"
    log INFO "SSH keys generated"
  else
    log INFO "Using pre-generated SSH keys"
  fi
  
  # Start dropbear SSH server
  log INFO "Starting dropbear on port $SSHD_PORT..."
  EPKA_LIB="$RUNCVM_GUEST/tmp/dropbear/libepka_file.so"
  
  if [ -f "$EPKA_LIB" ]; then
    "$RUNCVM_LD" "$RUNCVM_GUEST/usr/sbin/dropbear" -REF -p "$SSHD_PORT" \
      -A "$EPKA_LIB,$DROPBEAR_DIR/epka.json" \
      -P "$DROPBEAR_DIR/dropbear.pid" 2>/dev/null &
    sleep 0.5
    if [ -f "$DROPBEAR_DIR/dropbear.pid" ]; then
      log INFO "Dropbear started (PID: $(cat $DROPBEAR_DIR/dropbear.pid))"
    else
      log INFO "Warning - Dropbear may not have started correctly"
    fi
  else
    log INFO "Warning - EPKA library not found at $EPKA_LIB"
    log INFO "docker exec may not work"
  fi
else
  log INFO "RunCVM tools not available, skipping dropbear"
  log INFO "docker exec will not work"
fi

log INFO "========== DROPBEAR SETUP END =========="

# Note: Static watch binary (from procps) is copied to /usr/bin/watch during rootfs staging
# This provides proper Ctrl-C handling unlike busybox watch

# ========== TTY AND SIGNAL SETUP ==========
# This is critical for proper signal handling (Ctrl-C, Ctrl-Z, etc.)
# Without this, interactive commands like 'watch' won't respond to signals.
#
# IMPORTANT: We cannot use setsid with exec because:
# - setsid forks, parent exits immediately -> kernel panic (PID 1 cannot exit)
#
# Instead, we:
# 1. Redirect I/O to serial console for proper terminal
# 2. Run entrypoint as a child process (not exec)
# 3. Set up trap to forward signals to child
# 4. Wait for child to finish
# 5. Trigger proper shutdown (reboot -f) - PID 1 must never exit normally

# Determine what to run
# Priority:
# 1. /.runcvm-entrypoint (saved by RunCVM)
# 2. /docker-entrypoint.sh (nginx and many others)
# 3. Direct nginx execution
# 4. /bin/sh fallback

# TTY Handling Strategy:
# 1. cttyhack (Busybox) - Best, designed for this exactly
# 2. setsid (util-linux/Busybox) - Good, creates new session. 
#    Need to explicitly open /dev/console to make it the controlling terminal.

CTTYHACK=""
if command -v cttyhack >/dev/null 2>&1; then
  CTTYHACK="cttyhack"
elif command -v busybox >/dev/null 2>&1 && busybox --list | grep -q cttyhack; then
  CTTYHACK="busybox cttyhack"
elif [ -x "/bin/busybox" ] && /bin/busybox --list | grep -q cttyhack; then
  CTTYHACK="/bin/busybox cttyhack"
elif [ -x "/.runcvm/guest/bin/busybox" ] && /.runcvm/guest/bin/busybox --list | grep -q cttyhack; then
  if [ -x "/.runcvm/guest/lib/ld" ]; then
    CTTYHACK="/.runcvm/guest/lib/ld /.runcvm/guest/bin/busybox cttyhack"
  else
    CTTYHACK="/.runcvm/guest/bin/busybox cttyhack"
  fi
fi

if [ -n "$CTTYHACK" ]; then
  # Option 1: cttyhack found
  if [ "$RUNCVM_LOG_LEVEL" = "DEBUG" ]; then
    echo "RunCVM-FC init: TTY enabled ($CTTYHACK)"
  fi
  
  run_with_tty() {
    # Run as child so we can reboot after
    $CTTYHACK "$@"
    return $?
  }
elif command -v setsid >/dev/null 2>&1; then
  # Option 2: setsid found
  if [ "$RUNCVM_LOG_LEVEL" = "DEBUG" ]; then
    echo "RunCVM-FC init: TTY enabled (setsid fallback)"
  fi
  
  # Wrapper to run in new session and acquire controlling terminal
  # We exec a shell that opens console (becoming ctty) and then execs the target
  # We do NOT exec the setid command itself, so PID 1 stays alive to reboot
  # Note: setsid -c sets the controlling terminal to stdin (which we redirect/open properly)
  run_with_tty() {
    setsid -c sh -c 'exec "$@" <> /dev/ttyS0 >&0 2>&1' -- "$@"
    return $?
  }
else
  # Option 3: No TTY tools
  echo "RunCVM-FC init: WARNING - No TTY tools found (cttyhack/setsid)"
  echo "RunCVM-FC init: Interactive shells may not work correctly"
  
  run_with_tty() {
    "$@"
    return $?
  }
fi

if [ -f /.runcvm-entrypoint ] && [ -s /.runcvm-entrypoint ]; then
  # Read entrypoint line by line into an array-like structure
  set --
  while IFS= read -r line || [ -n "$line" ]; do
    set -- "$@" "$line"
  done < /.runcvm-entrypoint
  
  echo "RunCVM-FC init: Running saved entrypoint: $@"
  run_with_tty "$@"
  
elif [ -x /docker-entrypoint.sh ]; then
  echo "RunCVM-FC init: Running /docker-entrypoint.sh"
  if [ -f /etc/nginx/nginx.conf ]; then
    run_with_tty /docker-entrypoint.sh nginx -g "daemon off;"
  else
    run_with_tty /docker-entrypoint.sh
  fi
  
elif [ -f /etc/nginx/nginx.conf ] && command -v nginx >/dev/null 2>&1; then
  echo "RunCVM-FC init: Running nginx directly"
  run_with_tty nginx -g "daemon off;"
  
else
  echo "RunCVM-FC init: No entrypoint found, starting shell"
  run_with_tty /bin/sh
fi

# We should only get here if run_with_tty failed to exec (e.g. command not found)
# OR if run_with_tty was not an exec (which it is currently)
RET=$?
echo "RunCVM-FC init: Entrypoint exited with code $RET"
busybox reboot -f
INITEOF
  busybox chmod +x "$init_script"
  
  # Save the original entrypoint if we have it
  if [ -f "$entrypoint_file" ]; then
    busybox cp "$entrypoint_file" "${staging_dir}/.runcvm-entrypoint"
    log "  Saved entrypoint: $(busybox head -1 ${staging_dir}/.runcvm-entrypoint)"
  fi
  
  # Copy essential networking tools from RunCVM guest
  # These are needed because minimal images (like nginx) don't have ip/ifconfig
  # IMPORTANT: We must copy to the SAME path (/.runcvm/guest) because the binaries
  # use relative RPATH that depends on the directory structure
  log "  Copying network tools to rootfs..."
  
  if [ -d "$RUNCVM_GUEST" ]; then
    # Create the same directory structure
    busybox mkdir -p "${staging_dir}/.runcvm/guest"
    
    # Copy the dynamic linker and libraries
    if [ -d "$RUNCVM_GUEST/lib" ]; then
      busybox cp -a "$RUNCVM_GUEST/lib" "${staging_dir}/.runcvm/guest/"
      log "    Copied lib/ (dynamic linker)"
    fi
    
    # Copy usr/lib for additional libraries
    if [ -d "$RUNCVM_GUEST/usr/lib" ]; then
      busybox mkdir -p "${staging_dir}/.runcvm/guest/usr"
      busybox cp -a "$RUNCVM_GUEST/usr/lib" "${staging_dir}/.runcvm/guest/usr/"
      log "    Copied usr/lib/"
    fi
    
    # Copy bin directory with busybox and symlinks
    if [ -d "$RUNCVM_GUEST/bin" ]; then
      busybox cp -a "$RUNCVM_GUEST/bin" "${staging_dir}/.runcvm/guest/"
      log "    Copied bin/ (busybox, ip, etc.)"
    fi
    
    # Copy sbin if exists
    if [ -d "$RUNCVM_GUEST/sbin" ]; then
      busybox cp -a "$RUNCVM_GUEST/sbin" "${staging_dir}/.runcvm/guest/"
      log "    Copied sbin/"
    fi
    
    # Copy usr/bin and usr/sbin
    if [ -d "$RUNCVM_GUEST/usr/bin" ]; then
      busybox mkdir -p "${staging_dir}/.runcvm/guest/usr"
      busybox cp -a "$RUNCVM_GUEST/usr/bin" "${staging_dir}/.runcvm/guest/usr/"
      log "    Copied usr/bin/"
    fi
    if [ -d "$RUNCVM_GUEST/usr/sbin" ]; then
      busybox mkdir -p "${staging_dir}/.runcvm/guest/usr"
      busybox cp -a "$RUNCVM_GUEST/usr/sbin" "${staging_dir}/.runcvm/guest/usr/"
      log "    Copied usr/sbin/"
    fi
    
    # Copy usr/share (contains terminfo for ncurses programs like watch)
    if [ -d "$RUNCVM_GUEST/usr/share" ]; then
      busybox mkdir -p "${staging_dir}/.runcvm/guest/usr"
      busybox cp -a "$RUNCVM_GUEST/usr/share" "${staging_dir}/.runcvm/guest/usr/"
      log "    Copied usr/share/ (terminfo)"
    fi
    
    # Copy tmp/dropbear (contains EPKA library for SSH authentication)
    if [ -d "$RUNCVM_GUEST/tmp/dropbear" ]; then
      busybox mkdir -p "${staging_dir}/.runcvm/guest/tmp"
      busybox cp -a "$RUNCVM_GUEST/tmp/dropbear" "${staging_dir}/.runcvm/guest/tmp/"
      log "    Copied tmp/dropbear/ (EPKA library)"
    fi
    
    # Copy scripts directory (needed for runcvm-vm-exec)
    if [ -d "$RUNCVM_GUEST/scripts" ]; then
      busybox cp -a "$RUNCVM_GUEST/scripts" "${staging_dir}/.runcvm/guest/"
      log "    Copied scripts/"
    fi
  else
    log "  WARNING: RUNCVM_GUEST not found, network tools may not work"
  fi
  
  # Install procps watch wrapper (overrides busybox watch)
  # This provides proper Ctrl-C handling
  # The bundled watch uses BUNDELF and needs the dynamic linker at /.runcvm/guest/lib/ld
  log "  Checking for static watch binary..."
  
  WATCH_BIN=""
  for path in "/.runcvm/guest/bin/watch" "/.runcvm/bin/watch" "/opt/runcvm/bin/watch"; do
    if [ -f "$path" ]; then
      WATCH_BIN="$path"
      break
    fi
  done
  
  if [ -n "$WATCH_BIN" ]; then
    # Remove busybox symlink first (it's at /bin/watch)
    busybox rm -f "${staging_dir}/bin/watch" 2>/dev/null || true
    busybox rm -f "${staging_dir}/usr/bin/watch" 2>/dev/null || true
    
    # Create wrapper script that uses BUNDELF dynamic linker and terminfo
    busybox mkdir -p "${staging_dir}/bin"
    busybox cat > "${staging_dir}/bin/watch" << 'WATCHEOF'
#!/bin/sh
# Wrapper for procps watch (uses BUNDELF dynamic linker)
# Provides proper Ctrl-C handling unlike busybox watch
export TERMINFO=/.runcvm/guest/usr/share/terminfo
exec /.runcvm/guest/lib/ld /.runcvm/guest/bin/watch "$@"
WATCHEOF
    busybox chmod +x "${staging_dir}/bin/watch"
    log "    ✓ Installed procps watch wrapper to /bin/watch"
  else
    log "    ✗ Bundled watch not found"
  fi
  
  # Copy pre-generated dropbear keys from container (generated by runcvm-ctr-entrypoint)
  if [ -d "/.runcvm/dropbear" ] && [ -f "/.runcvm/dropbear/key" ]; then
    busybox mkdir -p "${staging_dir}/.runcvm/dropbear"
    busybox cp -a "/.runcvm/dropbear/"* "${staging_dir}/.runcvm/dropbear/"
    log "    Copied pre-generated dropbear keys"
  fi
  
  # Copy the config file (needed by some scripts)
  if [ -f "/.runcvm/config" ]; then
    busybox cp "/.runcvm/config" "${staging_dir}/.runcvm/config"
    log "    Copied /.runcvm/config"
  fi
  # Copy nfs-mounts file - CRITICAL for NFS volume mounting in VM
  log_debug "Checking NFS_CONFIG at $NFS_CONFIG"
  if [ -f "$NFS_CONFIG" ]; then
    log_debug "NFS_CONFIG exists, copying to staging..."
    if [ -s "$NFS_CONFIG" ]; then
      busybox cp "$NFS_CONFIG" "${staging_dir}/.runcvm/nfs-mounts"
      log_debug "Copied to ${staging_dir}/.runcvm/nfs-mounts"
      log_debug "Contents:"
      busybox cat "${staging_dir}/.runcvm/nfs-mounts" 2>&1 | while read line; do log_debug "  $line"; done
    else
      log "WARNING: NFS_CONFIG exists but is empty"
    fi
  else
    log "WARNING: NFS_CONFIG does not exist at $NFS_CONFIG"
  fi

  # Copy kernel modules for 9P support from Alpine kernel
  log "  Copying 9P kernel modules..."
  local fc_modules_dir="$RUNCVM_GUEST/kernels/firecracker/latest/modules"
  if [ -d "$fc_modules_dir" ]; then
    # Find the kernel version directory
    local kernel_ver=$(busybox ls "$fc_modules_dir" 2>/dev/null | busybox head -1)
    if [ -n "$kernel_ver" ] && [ -d "$fc_modules_dir/$kernel_ver" ]; then
      log "    Found kernel modules for version: $kernel_ver"
      
      # Create modules directory in staging
      busybox mkdir -p "${staging_dir}/lib/modules/$kernel_ver"
      
      # Copy 9P modules specifically
      if [ -d "$fc_modules_dir/$kernel_ver/kernel/net/9p" ]; then
        busybox mkdir -p "${staging_dir}/lib/modules/$kernel_ver/kernel/net"
        busybox cp -a "$fc_modules_dir/$kernel_ver/kernel/net/9p" \
          "${staging_dir}/lib/modules/$kernel_ver/kernel/net/"
        log "      Copied 9P modules"
      fi
      
      # Copy vsock modules if they exist
      if [ -d "$fc_modules_dir/$kernel_ver/kernel/net/vmw_vsock" ]; then
        busybox mkdir -p "${staging_dir}/lib/modules/$kernel_ver/kernel/net"
        busybox cp -a "$fc_modules_dir/$kernel_ver/kernel/net/vmw_vsock" \
          "${staging_dir}/lib/modules/$kernel_ver/kernel/net/"
        log "      Copied vsock modules"
      fi
      
      # Copy modules.* files (needed for modprobe)
      busybox cp "$fc_modules_dir/$kernel_ver"/modules.* \
        "${staging_dir}/lib/modules/$kernel_ver/" 2>/dev/null || true
      log "      Copied module metadata files"
    else
      log "    WARNING: No kernel modules found in $fc_modules_dir"
    fi
  else
    log "    WARNING: Firecracker modules directory not found: $fc_modules_dir"
  fi

  
  # Create a simpler runcvm-vm-exec for Firecracker that doesn't depend on complex paths
  # This is called by dropbear when docker exec connects via SSH
  busybox cat > "${staging_dir}/.runcvm/guest/scripts/runcvm-vm-exec" << 'VMEXECEOF'
#!/bin/sh
# Simplified runcvm-vm-exec for Firecracker
# Called by dropbear SSH when docker exec connects
# Arguments: <uid:gid:groups> <cwd_encoded> <args_encoded> <env_encoded>

from_bin() {
  tr '\200\201\202\203\204\205' "\011\012\040\047\042\134"
}

uidgid="$1"
cwd_bin="$2"
args_bin="$3"
env_bin="$4"

# Decode working directory
cwd=$(printf '%s' "$cwd_bin" | from_bin)

# Change to working directory
cd "$cwd" 2>/dev/null || cd /

# Decode and execute the command
# The args are newline-separated after decoding
if [ -n "$args_bin" ]; then
  # Create a temporary script to handle the decoded args properly
  tmpscript="/tmp/exec-$$"
  printf '%s' "$args_bin" | from_bin > "$tmpscript.args"
  
  # Read first line as command, rest as args
  cmd=""
  args=""
  first=1
  while IFS= read -r line || [ -n "$line" ]; do
    if [ "$first" = "1" ]; then
      cmd="$line"
      first=0
    else
      if [ -z "$args" ]; then
        args="$line"
      else
        args="$args
$line"
      fi
    fi
  done < "$tmpscript.args"
  rm -f "$tmpscript.args"
  
  # Execute the command
  if [ -n "$args" ]; then
    # Multiple arguments - use eval to handle them
    printf '%s\n' "$args" | {
      set -- "$cmd"
      while IFS= read -r arg || [ -n "$arg" ]; do
        set -- "$@" "$arg"
      done
      exec "$@"
    }
  else
    # Single command
    exec "$cmd"
  fi
else
  exec /bin/sh
fi
VMEXECEOF
  busybox chmod +x "${staging_dir}/.runcvm/guest/scripts/runcvm-vm-exec"
  log "    Created simplified runcvm-vm-exec"
  
  # Save network configuration for the VM
  # Save network configuration for all VMs
  if [ -d "/.runcvm/network/devices" ]; then
    for net_dev_file in $(busybox ls /.runcvm/network/devices/* | busybox sort); do
      local ifname=$(busybox basename "$net_dev_file")
      [ "$ifname" = "default" ] && continue
      
      read DOCKER_IF DOCKER_IF_MAC DOCKER_IF_MTU DOCKER_IF_IP DOCKER_IF_IP_NETPREFIX DOCKER_IF_IP_GW < "$net_dev_file"
      
      # Firecracker MAC
      local fc_mac=$(echo "$DOCKER_IF_MAC" | busybox sed 's/^..:..:../AA:FC:00/')
      
      log "  Saving network config for $ifname: IP=$DOCKER_IF_IP/$DOCKER_IF_IP_NETPREFIX GW=$DOCKER_IF_IP_GW"
      
      cat > "${staging_dir}/.runcvm-network-${ifname}" << NETEOF
FC_IP="$DOCKER_IF_IP"
FC_PREFIX="$DOCKER_IF_IP_NETPREFIX"
FC_GW="$DOCKER_IF_IP_GW"
FC_MTU="$DOCKER_IF_MTU"
FC_MAC="$fc_mac"
NETEOF
    done
  fi
  
  # Save resolv.conf
  if [ -f "/etc/resolv.conf" ]; then
    busybox cp /etc/resolv.conf "${staging_dir}/.runcvm-resolv.conf"
  fi
  
  # Step 2: Calculate rootfs size
  # Add 50% overhead + 256MB for ext4 metadata
  local rootfs_size=$(( (staged_size * 3 / 2) + 256 ))
  [ "$rootfs_size" -lt 512 ] && rootfs_size=512
  
  log "Step 2/3: Creating rootfs image..."
  log "  Rootfs size: ${rootfs_size}MB (${staged_size}MB content + overhead)"
  
  # Check overlay disk space
  local overlay_avail=$(busybox df -m / 2>/dev/null | busybox awk 'NR==2 {print $4}')
  log "  Overlay (/) available: ${overlay_avail}MB"
  
  if [ "$overlay_avail" -lt "$rootfs_size" ]; then
    busybox rm -rf "$staging_dir"
    error "Insufficient disk space on overlay (need ${rootfs_size}MB, have ${overlay_avail}MB)"
  fi
  
  # Create rootfs from staging directory
  log "  Source: $staging_dir"  
  log "  Destination: $rootfs_path"
  
  local mke2fs_start=$(busybox date +%s)
  
  if ! create_rootfs_from_dir "$staging_dir" "$rootfs_path" "$rootfs_size"; then
    log "mke2fs debug info:"
    log "  Staged content size: $(busybox du -sm "$staging_dir" | busybox cut -f1)MB"
    log "  Image size: ${rootfs_size}MB"
    busybox rm -rf "$staging_dir"
    error "Failed to create rootfs"
  fi
  
  local mke2fs_end=$(busybox date +%s)
  local mke2fs_time=$(( mke2fs_end - mke2fs_start ))
  
  # Cleanup staging
  log "  Cleaning up staging area..."
  busybox rm -rf "$staging_dir"
  
  local final_size=$(busybox du -sm "$rootfs_path" 2>/dev/null | busybox cut -f1)
  log "  Rootfs ready: ${final_size}MB (created in ${mke2fs_time}s)"
  
  # Determine kernel path - using Firecracker kernel Image (uncompressed)
  # The kernel is copied to /opt/runcvm/kernels/firecracker/vmlinux in Dockerfile
  # which becomes /.runcvm/guest/kernels/firecracker/vmlinux at runtime
  local fc_default_kernel="$RUNCVM_GUEST/kernels/firecracker/vmlinux"
  local kernel_path="${RUNCVM_FC_KERNEL_PATH:-$fc_default_kernel}"
  
  if [ ! -f "$kernel_path" ]; then
    error "Firecracker kernel not found: $kernel_path"
  fi
  
  # DEBUG: Show kernel file details
  log "DEBUG: Kernel file information:"
  log "  Path: $kernel_path"
  log "  Size: $(busybox ls -lh "$kernel_path" 2>/dev/null | busybox awk '{print $5}')"
  log "  Modified: $(busybox ls -l "$kernel_path" 2>/dev/null | busybox awk '{print $6, $7, $8}')"
  if command -v sha256sum >/dev/null 2>&1; then
    log "  SHA256: $(sha256sum "$kernel_path" 2>/dev/null | busybox cut -d' ' -f1)"
  fi
  
  # Build boot arguments - use /init which we created in the rootfs
  # Note: 9p.debug=0xff enables verbose 9P logging to debug transport issues
  local boot_args="init=/init console=ttyS0 reboot=k panic=1 pci=off root=/dev/vda rw 9p.debug=0xff"
  
  # Add 'quiet' to suppress kernel boot messages when not in DEBUG mode
  if [ "$RUNCVM_LOG_LEVEL" != "DEBUG" ]; then
    boot_args="init=/init console=ttyS0 quiet reboot=k panic=1 pci=off root=/dev/vda rw"
  fi
  
  # Parse memory size (remove 'M' suffix if present)
  local mem_mb="${RUNCVM_MEM_SIZE%M}"
  [ -z "$mem_mb" ] && mem_mb=768
  
  # Parse CPU count
  local vcpu_count="${RUNCVM_CPUS:-1}"
  [ "$vcpu_count" -le 0 ] && vcpu_count=$(busybox nproc)
  
  log "Step 3/3: Starting Firecracker VM..."
  log "  Kernel: $kernel_path"
  log "  Config: $vcpu_count vCPUs, ${mem_mb}MB RAM"
  
  # Build network config if available
  local network_interfaces_json=""
  local is_host_mode=0
  [ -f "/.runcvm/network/host_mode" ] && is_host_mode=1
  
  # Iterate over all defined network devices
  # We use a sorted list to ensure eth0 comes first (important for default gateway)
  if [ -d "/.runcvm/network/devices" ]; then
    for device_file in $(busybox ls /.runcvm/network/devices/* | busybox sort); do
      # Skip the 'default' symlink to avoid duplicates
      if [ "$(busybox basename "$device_file")" = "default" ]; then
        continue
      fi
      
      read DOCKER_IF DOCKER_IF_MAC DOCKER_IF_MTU DOCKER_IF_IP DOCKER_IF_IP_NETPREFIX DOCKER_IF_IP_GW < "$device_file"
      
      # For Firecracker, we need a unique MAC for the guest side.
      # We modify the container's MAC to start with AA:FC:00 so it's distinct but related.
      # If we have multiple interfaces, this simple sed might map them to the same MAC if the suffix is identical.
      # But typically container MACs are unique.
      local fc_mac=$(echo "$DOCKER_IF_MAC" | busybox sed 's/^..:..:../AA:FC:00/')
      
      local tap_name=""
      
      if [ "$is_host_mode" = "1" ] && [ "$DOCKER_IF" = "eth0" ]; then
        # Host Mode (NAT/TAP)
        # Entrypoint already created 'tap0' for us
        tap_name="tap0"
        log "  Network ($DOCKER_IF): Host Mode, using existing $tap_name"
        
        # Ensure it's up
        ip link set dev "$tap_name" up mtu "${DOCKER_IF_MTU:-1500}"
        
      else
        # Bridge Mode (Docker/Kubernetes)
        tap_name="tap-$DOCKER_IF"
        local bridge_name="br-$DOCKER_IF"
        
        log "  Network ($DOCKER_IF): $tap_name ($fc_mac) -> $bridge_name"
        
        # Create TAP device for Firecracker if it doesn't exist
        if ! ip link show "$tap_name" >/dev/null 2>&1; then
           log "    Creating TAP device $tap_name..."
           ip tuntap add dev "$tap_name" mode tap
        fi
        
        # Add TAP to the bridge
        log "    Adding $tap_name to bridge $bridge_name..."
        ip link set dev "$tap_name" master "$bridge_name"
        
        # Set MTU and bring up
        ip link set dev "$tap_name" up mtu "${DOCKER_IF_MTU:-1500}"
      
        # Debug: show bridge and tap status
        if [ "$RUNCVM_LOG_LEVEL" = "DEBUG" ]; then
          log "    Bridge status:"
          ip link show "$bridge_name" 2>&1 | busybox sed 's/^/      /'
          log "    TAP status:"
          ip link show "$tap_name" 2>&1 | busybox sed 's/^/      /'
        fi
        
        # Add MASQUERADE rule for outbound traffic from the VM (for the default gateway interface)
        # This allows the VM to reach external networks (internet)
        if [ -n "$DOCKER_IF_IP" ] && [ "$DOCKER_IF_IP" != "-" ]; then
             log "    Adding NAT MASQUERADE rule for $DOCKER_IF..."
             if [ -n "$RUNCVM_GUEST" ] && [ -d "$RUNCVM_GUEST/usr/lib/xtables" ]; then
               XTABLES_LIBDIR=$RUNCVM_GUEST/usr/lib/xtables xtables-nft-multi iptables \
                 -t nat -A POSTROUTING -s "$DOCKER_IF_IP" -o "$DOCKER_IF_IP" -j MASQUERADE 2>/dev/null || true
                 # Note: The original code used -o eth0 (hardcoded). 
                 # In bridge mode, the traffic leaves via the container's eth0 usually?
                 # Actually, for the VM, traffic leaves via TAP -> Bridge -> Container Eth0.
                 # The NAT rule is needed if the *container* needs to NAT VM traffic out to the world.
                 # Originally: -s "$DOCKER_IF_IP" -o eth0.
                 # If this is eth0, then -o eth0 is correct (BUT wait, eth0 is now a slave of br-eth0).
                 # So traffic leaves via br-eth0? No, traffic leaves via the bridge routing?
                 #
                 # Wait, in the entrypoint, we moved the Container IP to br-eth0?
                 # Entrypoint: `ip addr flush dev eth0`, `ip addr add ... dev br-eth0`.
                 # So the container's "external" interface is now br-eth0.
                 #
                 # But we usually want to NAT traffic *leaving* the container infrastructure?
                 # Actually, usually Docker handles the NAT for the container.
                 # This rule seems to be for traffic *originating from the VM* (which has FC_IP which is DOCKER_IF_IP).
                 #
                 # Wait, the VM uses the SAME IP as the container (`DOCKER_IF_IP`).
                 # So effectively the VM *is* the container on the network.
                 # So we don't need NAT for the VM to be seen as the container IP.
                 #
                 # The original code:
                 # iptables -t nat -A POSTROUTING -s "$DOCKER_IF_IP" -o eth0 -j MASQUERADE
                 # This masquerades traffic FROM the IP (VM/Container IP) going out eth0.
                 # This looks weird. If source is IP, why masquerade?
                 # Maybe to fix some routing issue with the bridge?
                 #
                 # In multi-nic setup, if we have eth0 and eth1.
                 # We should probably skip this explicit NAT rule if we are confident the bridge routing works.
                 # But let's assume valid: if valid, it should likely be:
                 # -o eth0 (if that's the upstream).
                 # If we have multiple NICs, which is upstream?
                 # Usually the default gateway interface. DOCKER_IF_IP_GW tells us.
                 
                 # Let's keep it simple: if this interface has a Gateway, we add the rule.
                 if [ -n "$DOCKER_IF_IP_GW" ] && [ "$DOCKER_IF_IP_GW" != "-" ]; then
                    # This is likely the default route interface.
                    # The physical interface is $DOCKER_IF (now enslaved to br-$DOCKER_IF).
                    # But the *route* probably goes via ... wait.
                    # If we enslaved eth0 to br-eth0, traffic goes out via br-eth0?
                    # The bridge *is* the interface with the IP.
                    
                    # Let's keep the existing logic but make it dynamic. 
                    # Use -j MASQUERADE for all traffic from this IP leaving the container?
                    # Actually, let's just stick to the original "eth0" logic for "eth0" interface only for safety,
                    # or apply to all?
                    
                    # Original code was hardcoded to eth0.
                    # Currently we iterate. If DOCKER_IF == eth0, we can add it.
                    true
                 fi
             fi
        fi
      fi
      
      # Build JSON object for this interface
      local iface_json="{\"iface_id\":\"$DOCKER_IF\",\"guest_mac\":\"$fc_mac\",\"host_dev_name\":\"$tap_name\"}"
      
      if [ -z "$network_interfaces_json" ]; then
        network_interfaces_json="$iface_json"
      else
        network_interfaces_json="$network_interfaces_json,$iface_json"
      fi
      
    done
  fi # if devices dir exists

  # Wrap in array structure if we found interfaces
  if [ -n "$network_interfaces_json" ]; then
    network_config=",\"network-interfaces\":[$network_interfaces_json]"
  fi

  # Build vsock config for FUSE volumes
  # FORCE ENABLE vsock for debugging - always add the vsock device
  local vsock_config=',"vsock":{"guest_cid":3,"uds_path":"/run/firecracker.vsock"}'
  log "  vsock: FORCE ENABLED (debugging)"
  log "    guest_cid: 3"
  log "    uds_path: /run/firecracker.vsock"
  
  # Create Firecracker config file
  cat > "$FIRECRACKER_CONFIG" << CFGEOF
{
  "boot-source": {
    "kernel_image_path": "$kernel_path",
    "boot_args": "$boot_args"
  },
  "logger": {
    "log_path": "/dev/null",
    "level": "Error",
    "show_level": false,
    "show_log_origin": false
  },
  "drives": [
    {
      "drive_id": "rootfs",
      "path_on_host": "$rootfs_path",
      "is_root_device": true,
      "is_read_only": false
    }
  ],
  "machine-config": {
    "vcpu_count": $vcpu_count,
    "mem_size_mib": $mem_mb
  }${network_config}${vsock_config}
}
CFGEOF

  # ALWAYS show Firecracker config for debugging vsock issue
  log "=== FIRECRACKER CONFIG (checking vsock) ==="
  busybox cat "$FIRECRACKER_CONFIG" | while read line; do log "  $line"; done
  log "=== END FIRECRACKER CONFIG ==="
  
  # Verify vsock is in config
  if busybox grep -q "vsock-device" "$FIRECRACKER_CONFIG"; then
    log "✓ vsock-device is PRESENT in Firecracker config"
  else
    log "✗ vsock-device is MISSING from Firecracker config!"
  fi

  # ============================================================
  # HOST TERMINAL CONFIGURATION
  # ============================================================
  # NOTE: We tried stty raw but it may trigger DSR queries.
  # The working test script runs Firecracker without modifying host terminal.
  # Let's try the same approach here.
  # ============================================================
  
  log "Starting Firecracker..."
  local fc_error_log=""

  if [ "$RUNCVM_LOG_LEVEL" = "DEBUG" ]; then
    # In DEBUG mode, show everything directly
    "$FIRECRACKER_BIN" --no-api --config-file "$FIRECRACKER_CONFIG"
  else
    # In non-DEBUG mode, suppress stderr (logs, banner) by redirecting to a temp file
    # This preserves stdout (console) interactivity while hiding the banner
    fc_error_log="/tmp/firecracker-error-$$.log"
    
    # Run Firecracker: stdout -> terminal (interactive), stderr -> log file
    "$FIRECRACKER_BIN" --no-api --config-file "$FIRECRACKER_CONFIG" 2> "$fc_error_log"
  fi
  
  local exit_code=$?

  # If failed and we have a log, show it
  if [ "$exit_code" -ne 0 ] && [ -n "$fc_error_log" ] && [ -f "$fc_error_log" ]; then
    cat "$fc_error_log" >&2
  fi
  
  # Cleanup temp log
  [ -n "$fc_error_log" ] && rm -f "$fc_error_log"
  
  log "Firecracker exited with code $exit_code"
  
  # Cleanup
  busybox rm -f "$rootfs_path" "$FIRECRACKER_CONFIG"
  
  return $exit_code
}

main "$@"
