#!/opt/runcvm/lib/ld-musl-aarch64.so.1 /opt/runcvm/bin/bash

# REFERENCES

# Qemu:
# - https://github.com/joshkunz/qemu-docker
# - https://mergeboard.com/blog/2-qemu-microvm-docker/
# - https://github.com/BBVA/kvm

# Virtiofs
# - https://vmsplice.net/~stefan/virtio-fs_%20A%20Shared%20File%20System%20for%20Virtual%20Machines.pdf
# - https://virtio-fs.gitlab.io/howto-qemu.html
# - https://www.tauceti.blog/posts/qemu-kvm-share-host-directory-with-vm-with-virtio/

# Container config.json spec
# - https://github.com/opencontainers/runtime-spec/
# - https://github.com/opencontainers/runtime-spec/blob/main/config.md

# Mount namespaces
# - https://www.kernel.org/doc/Documentation/filesystems/sharedsubtree.txt
# - https://www.redhat.com/sysadmin/mount-namespaces

RUNCVM=${RUNCVM:-/opt/runcvm}
if [ -x "$RUNCVM/lib/ld" ]; then
  RUNCVM_LD=$RUNCVM/lib/ld
else
  RUNCVM_LD=""
fi

if [ -x "$RUNCVM/usr/bin/jq" ]; then
  RUNCVM_JQ=$RUNCVM/usr/bin/jq
else
  RUNCVM_JQ="jq"
fi
RUNCVM_VM_MOUNTPOINT="/vm"
RUNCVM_GUEST=/.runcvm/guest
RUNCVM_ENTRYPOINT=$RUNCVM_GUEST/scripts/runcvm-ctr-entrypoint
RUNCVM_EXEC="$RUNCVM_GUEST/scripts/runcvm-ctr-exec"
RUNCVM_KERNELS=$RUNCVM/kernels
RUNCVM_GUEST_KERNELS=$RUNCVM_GUEST/kernels
RUNCVM_KERNEL_DEFAULT=firecracker
RUNCVM_MEM_SIZE_DEFAULT="768" # Default VM memory size (Mb) if not specified in container config
RUNCVM_CONTAINER_MEM_OVERHEAD="256" # Memory (Mb) overhead to allow for QEMU, virtiofsd, dnsmasq and other container memory footprint: 768Mb minimum, 1024Mb recommended

# ============================================================
# LOGGING SYSTEM
# Severity levels: DEBUG, INFO, ERROR, OFF
# Control via: RUNCVM_LOG_LEVEL environment variable
# ============================================================

# Default log level (can be overridden by environment)
RUNCVM_LOG_LEVEL="${RUNCVM_LOG_LEVEL:-OFF}"

# Log severity levels (numeric for comparison)
# Log severity levels (numeric for comparison)
get_log_level_num() {
  case "$1" in
    DEBUG) echo 0 ;;
    INFO|LOG) echo 1 ;;
    ERROR) echo 2 ;;
    OFF) echo 999 ;;
    *) echo 999 ;;
  esac
}

# Get numeric level for current log level (default to OFF=999)
CURRENT_LOG_LEVEL=$(get_log_level_num "${RUNCVM_LOG_LEVEL:-OFF}")

# Core logging function
_log() {
  local severity="$1"
  shift
  local message="$*"
  local severity_level=$(get_log_level_num "$severity")
  
  # Only log if severity meets threshold
  
  # Only log if severity meets threshold
  if [ "$severity_level" -ge "$CURRENT_LOG_LEVEL" ]; then
    local timestamp=$(date '+%Y-%m-%d %H:%M:%S.%6N')
    echo "[$timestamp] [RunCVM-Runtime] [$$] [$severity] $message" >> /tmp/runcvm-$$.log
    # Also output ERROR to stderr
    if [ "$severity" = "ERROR" ]; then
      echo "[$timestamp] [RunCVM-Runtime] [$severity] $message" >&2
    fi
  fi
}

# Convenience functions for different severity levels
log_debug() {
  _log DEBUG "$@"
}

log_info() {
  _log INFO "$@"
}

log() {
  # Default log function - maps to INFO for backward compatibility
  _log INFO "$@"
}

log_error() {
  _log ERROR "$@"
}

error() {
  # Skip past any docker error ending in CR
  (echo; echo) >&2
  
  # Dump message to stderr
  echo "RunCVM: Error: $1" >&2
  
  # Log error with severity
  log_error "$1"
  exit 1
}

# ============================================================
# CLEANUP HANDLER
# Ensures unfsd is stopped when runtime is terminated
# ============================================================

# Store container ID globally for cleanup
CLEANUP_CONTAINER_ID=""

cleanup_on_exit() {
  if [ -n "$CLEANUP_CONTAINER_ID" ] && [ -x /opt/runcvm/scripts/runcvm-nfsd ]; then
    echo "[CLEANUP] Stopping all unfsd instances for container $CLEANUP_CONTAINER_ID" >&2
    /opt/runcvm/scripts/runcvm-nfsd stop-all "$CLEANUP_CONTAINER_ID" 2>&1 || true
  fi
}

# Register cleanup handlers for signals
trap cleanup_on_exit EXIT SIGTERM SIGINT

load_env_from_file() {
  local file="$1"
  local var="$2"

  # Return gracefully if no $file exists
  if ! [ -f "$file" ]; then
    return 0
  fi

  while read LINE
  do
    local name="${LINE%%=*}"
    local value="${LINE#*=}"
    
    if [ "$name" != "$LINE" ] && [ "$value" != "$LINE" ] && [ "$name" = "$var" ]; then
      # We found variable $name: return it, removing any leading/trailing double quotes
      echo "$value" | sed 's/^"//;s/"$//'
      return 0
    fi
  done <"$file"
  
  return 1
}

jq() {
  $RUNCVM_LD $RUNCVM_JQ "$@"
}

jq_set() {
  local file="$1"
  shift
  
  local tmp="/tmp/config.json.$$"

  if jq "$@" $file >$tmp; then
    mv $tmp $file
  else
    echo "Failed to update $(basename $file); aborting!" 2>&1
    exit 1
  fi
}

jq_get() {
  local file="$1"
  shift
  
  jq -r "$@" $file
}

get_process_env() {
  local file="$1"
  local var="$2"
  local default="$3"
  local value
  
  value=$(jq_get "$file" --arg env "$var" '.env[] | select(match("^" + $env + "=")) | match("^" + $env + "=(.*)") | .captures[] | .string')
  
  [ -n "$value" ] && echo -n "$value" || echo -n "$default"
}

get_process_env_boolean() {
  local file="$1"
  local var="$2"
  local value
  
  value=$(jq_get "$file" --arg env "$var" '.env[] | select(match("^" + $env + "=")) | match("^" + $env + "=(.*)") | .captures[] | .string')
  
  [ -n "$value" ] && echo "1" || echo "0"
}

get_config_env() {
  local var="$1"
  local default="$2"
  local value

  value=$(jq_get "$CFG" --arg env "$var" '.process.env[] | select(match("^" + $env + "=")) | match("^" + $env + "=(.*)") | .captures[] | .string')
  
  [ -n "$value" ] && echo -n "$value" || echo -n "$default"
}

set_config_env() {
  local var="$1"
  local value="$2"
  
  jq_set "$CFG" --arg env "$var=$value" '.process.env |= (.+ [$env] | unique)'
}

# is_sandbox_container - Detect Kubernetes sandbox/pause containers
# These should bypass RunCVM and run directly with runc
# Returns 0 if sandbox container detected, 1 otherwise
is_sandbox_container() {
  local config_file="$1"
  
  # Check for Kubernetes CRI sandbox annotation
  # containerd sets this annotation for pause containers
  if jq_get "$config_file" '.annotations["io.kubernetes.cri.container-type"]' 2>/dev/null | grep -q "sandbox"; then
    log "Detected sandbox container via annotation io.kubernetes.cri.container-type=sandbox"
    return 0
  fi
  
  # Check for pause in image name (annotations may contain image info)
  local image_name=$(jq_get "$config_file" '.annotations["io.kubernetes.cri.image-name"]' 2>/dev/null)
  if echo "$image_name" | grep -qiE "(^|/)pause[:@]|/mirrored-pause[:@]"; then
    log "Detected sandbox container via image name: $image_name"
    return 0
  fi
  
  # Check process args for /pause binary
  local arg0=$(jq_get "$config_file" '.process.args[0]' 2>/dev/null)
  if [ "$arg0" = "/pause" ]; then
    log "Detected sandbox container via entrypoint: /pause"
    return 0
  fi
  
  # Check root path for pause-related paths (containerd naming)
  local root_path=$(jq_get "$config_file" '.root.path' 2>/dev/null)
  if echo "$root_path" | grep -qE "pause"; then
    log "Detected sandbox container via root path: $root_path"
    return 0
  fi

  # Check for io.kubernetes.cri.sandbox-id being equal to container id
  # (sandbox containers have their own id as sandbox-id)
  local sandbox_id=$(jq_get "$config_file" '.annotations["io.kubernetes.cri.sandbox-id"]' 2>/dev/null)
  local container_id=$(jq_get "$config_file" '.annotations["io.kubernetes.cri.container-id"]' 2>/dev/null)
  if [ -n "$sandbox_id" ] && [ "$sandbox_id" = "$container_id" ] && [ "$sandbox_id" != "null" ]; then
    log "Detected sandbox container via matching sandbox-id and container-id: $sandbox_id"
    return 0
  fi
  
  return 1
}


# PARSE RUNC GLOBAL OPTIONS:
# --debug             enable debug logging
# --log value         set the log file to write runc logs to (default is '/dev/stderr')
# --log-format value  set the log format ('text' (default), or 'json') (default: "text")
# --root value        root directory for storage of container state (this should be located in tmpfs) (default: "/run/user/1000/runc")
# --criu value        path to the criu binary used for checkpoint and restore (default: "criu")
# --systemd-cgroup    enable systemd cgroup support, expects cgroupsPath to be of form "slice:prefix:name" for e.g. "system.slice:runc:434234"
# --rootless value    ignore cgroup permission errors ('true', 'false', or 'auto') (default: "auto")

COMMAND_LINE=("$@")

log_debug "Command line: $0 ${COMMAND_LINE[@]@Q}"

while true
do
  case "$1" in
    --debug|--systemd-cgroup) shift; continue; ;;
    --log|--log-format|--root|--criu|--rootless) shift; shift; continue; ;;
    --log=*|--log-format=*|--root=*|--criu=*|--rootless=*) shift; continue; ;;
    *) break; ;;
  esac
done

COMMAND="$1"
shift

if [ "$COMMAND" = "create" ]; then

  log_debug "Command: create"
  
  # USAGE:
  #    runc create [command options] <container-id>
  #   
  # PARSE 'create' COMMAND OPTIONS
  # --bundle value, -b value  path to the root of the bundle directory, defaults to the current directory
  # --console-socket value    path to an AF_UNIX socket which will receive a file descriptor referencing the master end of the console's pseudoterminal
  # --pid-file value          specify the file to write the process id to
  # --no-pivot                do not use pivot root to jail process inside rootfs.  This should be used whenever the rootfs is on top of a ramdisk
  # --no-new-keyring          do not create a new session keyring for the container.  This will cause the container to inherit the calling processes session key
  # --preserve-fds value      Pass N additional file descriptors to the container (stdio + $LISTEN_FDS + N in total) (default: 0)
  while true
  do
    case "$1" in
      --bundle|-b) shift; BUNDLE="$1"; shift; continue; ;;
      --console-socket|--pid-file|--preserve-fds) shift; shift; continue; ;;
      --no-pivot|--no-new-keyring) shift; continue; ;;
      *) break; ;;
    esac
  done

  ID="$1"
  
  # Store ID for cleanup handler (in case of SIGTERM/SIGINT)
  CLEANUP_CONTAINER_ID="$ID"

  CFG="$BUNDLE/config.json"
  ROOT=$(jq -r .root.path $CFG)

  # Allow user to enable debug logging
  if [ "$(get_config_env RUNCVM_RUNTIME_DEBUG)" = "1" ]; then
    RUNCVM_LOG_LEVEL="DEBUG"
    CURRENT_LOG_LEVEL="${LOG_LEVELS[DEBUG]}"
  fi

  log_debug "Command line: $0 ${COMMAND_LINE[@]@Q}"
  log_debug "Command: create bundle=$BUNDLE id=$ID root=$ROOT"
  
  # Save formatted config.json
  jq -r . <$CFG >/tmp/config.json-$$-1

  # ============================================================
  # SANDBOX CONTAINER DETECTION
  # If this is a Kubernetes sandbox/pause container, bypass RunCVM
  # and pass directly to runc without modification
  # ============================================================
  if is_sandbox_container "$CFG"; then
    log "Sandbox container detected - bypassing RunCVM, passing to runc"
    exec /usr/bin/runc "${COMMAND_LINE[@]}"
  fi
  # ============================================================
  
  # Pending support for user-specified mountpoint for the guest (VM) binaries and scripts
  set_config_env "RUNCVM_GUEST" "$RUNCVM_GUEST"

  ARG0=$(jq_get "$CFG" '.process.args[0]')
  # Now look in mounts for destination == $ARG0 (this works for Docker and Podman)
  if [ "$ARG0" = "/sbin/docker-init" ] || [ "$ARG0" = "/dev/init" ]; then
  
    # User intended an init process to be run in the container,
    # so arrange to run our own instead, that will launch the original entrypoint
    
    # Look for and remove a mountpoint for this process.
    jq_set "$CFG" --arg init "$ARG0" '(.mounts[] | select(.destination == $init)) |= empty'
    
    # Replace the first argument with our own entrypoint; and remove the second, '--' (for now, #TODO)
    jq_set "$CFG" --arg entrypoint "$RUNCVM_ENTRYPOINT" '.process.args[0] = $entrypoint | del(.process.args[1])'
    
    # We know the user intended an init process to be run in the container.
    # TODO: We might want to indicate this, so that our entrypoint does not skip doing this
    # if the original entrypoint also looks like an init process.
    set_config_env "RUNCVM_INIT" "1"
  else
    # We don't know if the original entrypoint is an init process or not.
    # Run our entrypoint first to work this out and do the right thing.
    
    jq_set "$CFG" --arg entrypoint "$RUNCVM_ENTRYPOINT" '.process.args |= [$entrypoint] + .'
  fi

  # SET RUNCVM_HAS_HOME
  # 
  # If the HOME env var was not set either in the image, or via docker run, 
  # then it will be missing in the config env. Detect this case for communication to runcvm-ctr-entrypoint
  # so that HOME can be set to the requested user's default homedir.
  #
  # - See runcvm-ctr-entrypoint for full details of how/why hasHome is needed and HOME gets set.
  if [ -n "$(get_config_env HOME)" ]; then
    set_config_env "RUNCVM_HAS_HOME" "1"
  else
    set_config_env "RUNCVM_HAS_HOME" "0"
  fi

  # CONFIGURE USER
  # - Must be root to run container
  RUNCVM_UIDGID=$(jq_get "$CFG" '(.process.user.uid | tostring) + ":" + (.process.user.gid | tostring) + ":" + ((.process.user.additionalGids // []) | join(","))')
  set_config_env "RUNCVM_UIDGID" "$RUNCVM_UIDGID"
  jq_set "$CFG" '.process.user = {"uid":0, "gid":0}'
  log "RUNCVM_UIDGID=$RUNCVM_UIDGID"

  # CONFIGURE CPUS
  # CONFIGURE CPUS
  # Calculate CPUs based on quota and period
  # Default period is 100000 (100ms) if not specified
  CPU_QUOTA=$(jq_get "$CFG" '.linux.resources.cpu.quota')
  CPU_PERIOD=$(jq_get "$CFG" '.linux.resources.cpu.period // 100000')

  if [ "$CPU_QUOTA" = "null" ] || [ "$CPU_QUOTA" = "-1" ]; then
    # Unlimited CPUs
    RUNCVM_CPUS="0"
  else
    # Calculate ceil(quota / period) to ensure fractional CPUs get at least 1 vCPU
    # and e.g. 1.5 CPUs gets 2 vCPUs
    RUNCVM_CPUS=$(( ($CPU_QUOTA + $CPU_PERIOD - 1) / $CPU_PERIOD ))
    # Ensure at least 1 vCPU
    [ "$RUNCVM_CPUS" -lt 1 ] && RUNCVM_CPUS=1
  fi
  
  set_config_env "RUNCVM_CPUS" "$RUNCVM_CPUS"
  log "CPU Config: Quota=$CPU_QUOTA Period=$CPU_PERIOD -> RUNCVM_CPUS=$RUNCVM_CPUS"

  # CONFIGURE MOUNTS
  set_config_env "RUNCVM_VM_MOUNTPOINT" "$RUNCVM_VM_MOUNTPOINT"

  # First extract list of tmpfs mounts in fstab form, then delete them from the config
  RUNCVM_TMPFS=$(jq_get "$CFG" '( .mounts[] | select(.type == "tmpfs" and (.destination | test("^/dev(/|$)") | not) ) ) | [.source + " " + .destination + " tmpfs " + (.options | map(select(. != "rprivate" and . != "private")) | join(",")) + " 0 0"] | .[0]')
  jq_set "$CFG" -r 'del( .mounts[] | select(.type == "tmpfs" and (.destination | test("^/dev(/|$)") | not) ) )'
  set_config_env "RUNCVM_TMPFS" "$RUNCVM_TMPFS"

  # Rewrite all pre-existing bind/volume mounts (except those at or below /disks) to mount
  # below $RUNCVM_VM_MOUNTPOINT instead of below /.
  #
  # TODO TO CONSIDER:
  # If we excluded /etc/(resolv.conf,hosts,hostname), and moved these to top of the array
  # (by promoting them at the end of the below statements), they would be present in both
  # container and VM.
  #
  # N.B. A mount at or underneath /disks will NOT be mapped to /vm/disks - this path is reserved for mounting disk files to the container
  jq_set "$CFG" --arg vm "$RUNCVM_VM_MOUNTPOINT" '( .mounts[] | select(.type == "bind" and (.destination | test("^/disks(/|$)") | not) ) ).destination |= $vm + .'
  # ==========================================================================
  # CREATE VOLUMES FILE FOR FIRECRACKER 9P MOUNTS
  # ==========================================================================
  log "DEBUG: Checking RUNCVM_HYPERVISOR env..."
  HYPERVISOR="$(get_config_env RUNCVM_HYPERVISOR)"
  log "DEBUG: HYPERVISOR=$HYPERVISOR"
  log "DEBUG: ROOT=$ROOT"
  
  # Pass RUNCVM_NETWORK_MODE if present
  RUNCVM_NETWORK_MODE="$(get_config_env RUNCVM_NETWORK_MODE)"
  if [ -n "$RUNCVM_NETWORK_MODE" ]; then
    set_config_env "RUNCVM_NETWORK_MODE" "$RUNCVM_NETWORK_MODE"
  fi
  
  if [ "$HYPERVISOR" = "firecracker" ]; then
    VOLUMES_FILE="$ROOT/.runcvm/volumes"
    mkdir -p "$ROOT/.runcvm"
    > "$VOLUMES_FILE"
    
    log "FIRECRACKER: Creating volumes file for NFS..."
    [ "$RUNCVM_LOG_LEVEL" = "DEBUG" ] && echo "[DEBUG] FIRECRACKER: Creating volumes file for NFS..." >&2
    [ "$RUNCVM_LOG_LEVEL" = "DEBUG" ] && echo "[DEBUG] ROOT=$ROOT" >&2
    [ "$RUNCVM_LOG_LEVEL" = "DEBUG" ] && echo "[DEBUG] VOLUMES_FILE=$VOLUMES_FILE" >&2
    
    # Pick random port in range 1000-1050
    NFS_PORT=$((1000 + RANDOM % 51))
    
    # Get UID/GID from container config (defaults to 0:0)
    CONTAINER_UID=$(jq_get "$CFG" '.process.user.uid // 0')
    CONTAINER_GID=$(jq_get "$CFG" '.process.user.gid // 0')
    
    # Build NFS volumes string for environment variable
    NFS_VOLUMES=""
    
    # Use process substitution to avoid subshell (so NFS_PORT persists)
    while IFS=: read -r src dst; do
      [ -z "$src" ] && continue
      [ -z "$dst" ] && continue
      case "$dst" in
        /etc/resolv.conf|/etc/hosts|/etc/hostname) continue ;;
      esac
      case "$src" in
        */.runcvm*) continue ;;
      esac
      if [ -e "$src" ]; then
        log "FIRECRACKER:   Volume: $src -> $dst"
        [ "$RUNCVM_LOG_LEVEL" = "DEBUG" ] && echo "[DEBUG] FIRECRACKER: Volume: $src -> $dst (port $NFS_PORT)" >&2
        echo "${src}:${dst}:rw:${NFS_PORT}" >> "$VOLUMES_FILE"
        
        # Build env var (pipe-separated for multiple volumes)
        if [ -n "$NFS_VOLUMES" ]; then
          NFS_VOLUMES="${NFS_VOLUMES}|${src}:${dst}:${NFS_PORT}"
        else
          NFS_VOLUMES="${src}:${dst}:${NFS_PORT}"
        fi
        
        # Start NFS daemon on HOST for this volume
        if [ -x /opt/runcvm/scripts/runcvm-nfsd ]; then
          log "FIRECRACKER: Starting HOST unfsd for $src (port $NFS_PORT)..."
          [ "$RUNCVM_LOG_LEVEL" = "DEBUG" ] && echo "[DEBUG] Starting unfsd: $src port=$NFS_PORT uid=$CONTAINER_UID" >&2
          
          # Export RUNCVM_LOG_LEVEL so runcvm-nfsd can use it
          export RUNCVM_LOG_LEVEL
          /opt/runcvm/scripts/runcvm-nfsd start "$ID" "$src" "$dst" "$NFS_PORT" "$CONTAINER_UID" "$CONTAINER_GID" 2>&1
          NFS_PORT=$((NFS_PORT + 2))
        else
          log "WARNING: runcvm-nfsd not found at /opt/runcvm/scripts/runcvm-nfsd"
          [ "$RUNCVM_LOG_LEVEL" = "DEBUG" ] && echo "[DEBUG] ERROR: runcvm-nfsd not found!" >&2
        fi
      fi
    done < <(jq -r --arg vm "$RUNCVM_VM_MOUNTPOINT" '
      .mounts[] | 
      select(.type == "bind") | 
      select(.destination | startswith($vm + "/")) |
      "\(.source):\(.destination | ltrimstr($vm))"
    ' "$CFG" 2>/dev/null)
    
    # Pass NFS volumes via environment variable (bypasses file path issues)
    if [ -n "$NFS_VOLUMES" ]; then
      log "FIRECRACKER: Setting RUNCVM_NFS_VOLUMES=$NFS_VOLUMES"
      [ "$RUNCVM_LOG_LEVEL" = "DEBUG" ] && echo "[DEBUG] FIRECRACKER: Setting RUNCVM_NFS_VOLUMES=$NFS_VOLUMES" >&2
      set_config_env "RUNCVM_NFS_VOLUMES" "$NFS_VOLUMES"
    fi
    
    if [ -s "$VOLUMES_FILE" ]; then
      log "FIRECRACKER: Created volumes file with $(wc -l < "$VOLUMES_FILE") entries"
      [ "$RUNCVM_LOG_LEVEL" = "DEBUG" ] && echo "[DEBUG] FIRECRACKER: Volumes file created: $(cat "$VOLUMES_FILE")" >&2
    else
      log "FIRECRACKER: No user volumes found"
      [ "$RUNCVM_LOG_LEVEL" = "DEBUG" ] && echo "[DEBUG] FIRECRACKER: No user volumes found" >&2
      rm -f "$VOLUMES_FILE"
    fi
  fi
  # Mount / from container to $RUNCVM_VM_MOUNTPOINT, recursively binding all pre-existing mount points
  # (these being only the ones defined ahead of this item in the mounts[] array - so order matters!)
  jq_set "$CFG" --arg root "$ROOT" --arg vm "$RUNCVM_VM_MOUNTPOINT" '.mounts |= [{"destination":$vm,"type":"bind","source":$root,"options":["rbind","private","rw"]}] + .'

  # Mount /opt/runcvm from host to container
  # Define this at top of mounts[] so it is recursively mounted
  # and before (but after in the mounts[] array) /.runcvm so it can be mounted inside it
  jq_set "$CFG" --arg runcvm "$RUNCVM" --arg runcvm_guest "$RUNCVM_GUEST" '.mounts |= [{"destination":$runcvm_guest,"type":"bind","source":$runcvm,"options":["bind","private","ro"]}] + .'

  # Mount a tmpfs at /.runcvm in container
  # Define this at top of mounts[] so it is recursively mounted
  jq_set "$CFG" '.mounts |= [{"destination":"/.runcvm","type":"tmpfs","source":"runcvm","options":["nosuid","noexec","nodev","size=1M","mode=700"]}] + .'

  # Mount a tmpfs at /run in container
  # Define this at bottom of mounts[] so it is not recursively mounted to /vm
  jq_set "$CFG" '.mounts += [{"destination":"/run","type":"tmpfs","source":"run","options":["nosuid","noexec","nodev","size=1M","mode=700"]}]'

  # DETERMINE LAUNCH KERNEL:
  #
  # 1. If RUNCVM_KERNEL specified:
  #    - <dist> or <dist>/latest - use latest RUNCVM kernel available for this dist *and* ARGS
  #    - <dist>/<version> - use specific RUNCVM kernel version for this dist *and* ARGS
  # 2. Else, check /etc/os-release and:
  #    a. Use builtin kernel for this dist (if present in the expected location) *and* ARGS
  #    b. Use latest RUNCVM kernel available for the dist:
  #      - ID=alpine, VERSION_ID=3.16.0 => alpine/latest
  #      - ID=debian, VERSION_ID=11     => debian/latest
  #      - ID=ubuntu, VERSION_ID=22.04  => ubuntu/latest
  
  # Look for RUNCVM_KERNEL env var
  RUNCVM_KERNEL=$(get_config_env 'RUNCVM_KERNEL')
  log "RUNCVM_KERNEL='$RUNCVM_KERNEL' (1)"

  # Generate:
  # - RUNCVM_KERNEL_ID: the distro name (e.g. alpine, debian, ubuntu)
  # - RUNCVM_KERNEL_IDVER: the distro name and kernel version (e.g. alpine/5.15.59-0-virt, debian/5.10.0-16-amd64)

  if [ -n "$RUNCVM_KERNEL" ]; then
    # If found, validate
  
    if [[ "$RUNCVM_KERNEL" =~ \.\. ]]; then
      error "Kernel '$RUNCVM_KERNEL' invalid (contains '..')"
    fi
  
    if ! [[ "$RUNCVM_KERNEL" =~ ^[a-z]+(/[^/]+)?$ ]]; then
      error "Kernel '$RUNCVM_KERNEL' invalid (should match ^[a-z]+(/[^/]+)?$)"
    fi
  
    if ! [ -d "$RUNCVM_KERNELS/$RUNCVM_KERNEL" ]; then
      error "Kernel '$RUNCVM_KERNEL' not found (check $RUNCVM_KERNELS)"
    fi

    # If RUNCVM_KERNEL is a distro name only, append /latest
    if [[ "$RUNCVM_KERNEL" =~ ^[a-z]+$ ]]; then
      RUNCVM_KERNEL_IDVER="$RUNCVM_KERNEL/latest"
    else
      RUNCVM_KERNEL_IDVER="$RUNCVM_KERNEL"
    fi  

    RUNCVM_KERNEL_ID=$(dirname "$RUNCVM_KERNEL_IDVER") # Returns e.g. alpine, debian, ubuntu

  else
    # If not found, look for value from /etc/os-release in the container image
    
    RUNCVM_KERNEL_ID=$(load_env_from_file "$ROOT/etc/os-release" "ID")

    # Currently unused
    # RUNCVM_KERNEL_OS_VERSION_ID=$(load_var_from_env "$ROOT/etc/os-release" "VERSION_ID")

    # If still not found, assign a default
    if [ -z "$RUNCVM_KERNEL_ID" ]; then
      RUNCVM_KERNEL_ID="${RUNCVM_KERNEL_DEFAULT:-debian}"
    fi

    RUNCVM_KERNEL_IDVER="$RUNCVM_KERNEL_ID/latest"
  fi
  
  log "RUNCVM_KERNEL='$RUNCVM_KERNEL' (2)"
  log "RUNCVM_KERNEL_ID='$RUNCVM_KERNEL_ID'"
  log "RUNCVM_KERNEL_IDVER='$RUNCVM_KERNEL_IDVER'"
  
  # Now look up the default kernel and initramfs paths and args for this kernel
  case "$RUNCVM_KERNEL_ID" in
          debian) RUNCVM_KERNEL_OS_KERNEL_PATH="/vmlinuz"
                  RUNCVM_KERNEL_OS_INITRAMFS_PATH="/initrd.img"
                  RUNCVM_KERNEL_ROOT="rootfstype=virtiofs root=runcvmfs noresume net.ifnames=1"
                  ;;
          ubuntu) RUNCVM_KERNEL_OS_KERNEL_PATH="/boot/vmlinuz"
                  RUNCVM_KERNEL_OS_INITRAMFS_PATH="/boot/initrd.img"
                  RUNCVM_KERNEL_ROOT="rootfstype=virtiofs root=runcvmfs noresume net.ifnames=1"
                  ;;
              ol) RUNCVM_KERNEL_OS_KERNEL_PATH="/boot/vmlinuz"
                  RUNCVM_KERNEL_OS_INITRAMFS_PATH="/boot/initramfs"
                  RUNCVM_KERNEL_ROOT="root=virtiofs:runcvmfs noresume net.ifnames=1"
                  ;;
  alpine|openwrt) RUNCVM_KERNEL_OS_KERNEL_PATH="/boot/vmlinuz-virt"
                  RUNCVM_KERNEL_OS_INITRAMFS_PATH="/boot/initramfs-virt"
                  RUNCVM_KERNEL_ROOT="rootfstype=virtiofs root=runcvmfs resume="
                  ;;

           *) error "Unrecognised image O/S '$RUNCVM_KERNEL'; specify --env=RUNCVM_KERNEL=<dist> or --env=RUNCVM_KERNEL=<dist>/<version>"; ;;
  esac
  
  # If no RUNCVM_KERNEL specified, look for a kernel and initramfs at the expected paths in the container image.
  if [[ -z "$RUNCVM_KERNEL" && -f "$ROOT/$RUNCVM_KERNEL_OS_KERNEL_PATH" && -f "$ROOT/$RUNCVM_KERNEL_OS_INITRAMFS_PATH" ]]; then
    RUNCVM_KERNEL_PATH="$RUNCVM_KERNEL_OS_KERNEL_PATH"
    RUNCVM_KERNEL_INITRAMFS_PATH="$RUNCVM_KERNEL_OS_INITRAMFS_PATH"
  else
    # If RUNCVM_KERNEL was specified, or we didn't find a kernel and initramfs at the expected paths in the container image,
    # select the latest RUNCVM kernel version and arrange to mount it.

    # Check if Firecracker hypervisor is being used - skip module mounting since
    # Firecracker uses its own kernel with built-in modules (no external .ko files needed)
    RUNCVM_HYPERVISOR=$(get_config_env 'RUNCVM_HYPERVISOR')
    if [ "$RUNCVM_HYPERVISOR" = "firecracker" ]; then
      log "Firecracker hypervisor detected - skipping kernel module bind-mount"
      # Set dummy values for logging
      RUNCVM_KERNEL_VERSION="firecracker"
      RUNCVM_KERNEL_MODULES_SRC=""
      RUNCVM_KERNEL_MODULES_DST=""
    else
      RUNCVM_KERNEL_VERSION=$(basename $(readlink -f "$RUNCVM_KERNELS/$RUNCVM_KERNEL_IDVER")) # Returns e.g. 5.15.53-0-virt

      RUNCVM_KERNEL_MOUNT_LIB_MODULES=$(get_config_env 'RUNCVM_KERNEL_MOUNT_LIB_MODULES')
      if [ -n "$RUNCVM_KERNEL_MOUNT_LIB_MODULES" ]; then
        RUNCVM_KERNEL_MODULES_SRC="$RUNCVM_KERNELS/$RUNCVM_KERNEL_ID/$RUNCVM_KERNEL_VERSION/modules"
        RUNCVM_KERNEL_MODULES_DST="/lib/modules"
      else
        RUNCVM_KERNEL_MODULES_SRC="$RUNCVM_KERNELS/$RUNCVM_KERNEL_ID/$RUNCVM_KERNEL_VERSION/modules/$RUNCVM_KERNEL_VERSION"
        RUNCVM_KERNEL_MODULES_DST="/lib/modules/$RUNCVM_KERNEL_VERSION"
      fi
      
      RUNCVM_KERNEL_PATH="$RUNCVM_GUEST_KERNELS/$RUNCVM_KERNEL_ID/$RUNCVM_KERNEL_VERSION/vmlinuz"
      RUNCVM_KERNEL_INITRAMFS_PATH="$RUNCVM_GUEST_KERNELS/$RUNCVM_KERNEL_ID/$RUNCVM_KERNEL_VERSION/initrd"

      jq_set "$CFG" --arg modules_dst "$RUNCVM_VM_MOUNTPOINT$RUNCVM_KERNEL_MODULES_DST" --arg modules_src "$RUNCVM_KERNEL_MODULES_SRC" '.mounts += [{"destination":$modules_dst,"type":"bind","source":$modules_src,"options":["bind","private","ro"]}]'
    fi
  fi

  log "RUNCVM_KERNEL='$RUNCVM_KERNEL'"
  log "RUNCVM_KERNEL_ID='$RUNCVM_KERNEL_ID'"
  log "RUNCVM_KERNEL_VERSION='$RUNCVM_KERNEL_VERSION'"
  log "RUNCVM_KERNEL_OS_KERNEL_PATH='$RUNCVM_KERNEL_OS_KERNEL_PATH'"
  log "RUNCVM_KERNEL_OS_INITRAMFS_PATH='$RUNCVM_KERNEL_OS_INITRAMFS_PATH'"
  log "RUNCVM_KERNEL_PATH='$RUNCVM_KERNEL_PATH'"
  log "RUNCVM_KERNEL_INITRAMFS_PATH='$RUNCVM_KERNEL_INITRAMFS_PATH'"
  log "RUNCVM_KERNEL_ROOT='$RUNCVM_KERNEL_ROOT'"
  log "RUNCVM_KERNEL_MODULES_SRC='$RUNCVM_KERNEL_MODULES_SRC'"
  log "RUNCVM_KERNEL_MODULES_DST='$RUNCVM_KERNEL_MODULES_DST'"
  
  set_config_env "RUNCVM_KERNEL_PATH" "$RUNCVM_KERNEL_PATH"
  set_config_env "RUNCVM_KERNEL_INITRAMFS_PATH" "$RUNCVM_KERNEL_INITRAMFS_PATH"
  set_config_env "RUNCVM_KERNEL_ROOT" "$RUNCVM_KERNEL_ROOT"

  # Configure devices

  if [ "$(get_config_env 'RUNCVM_QEMU_NET_VHOST')" = "1" ]; then
    jq_set "$CFG" '.linux.resources.devices += [{"allow":true,"type":"c","major":10,"minor":232,"access":"rwm"},{"allow":true,"type":"c","major":10,"minor":200,"access":"rwm"},{"allow":true,"type":"c","major":10,"minor":238,"access":"rwm"}]'
    jq_set "$CFG" '.linux.devices+=[{"path":"/dev/net/tun","type":"c","major":10,"minor":200,"fileMode":8630,"uid":0,"gid":0},{"path":"/dev/kvm","type":"c","major":10,"minor":232,"fileMode":8630,"uid":0,"gid":0},{"path":"/dev/vhost-net","type":"c","major":10,"minor":238,"fileMode":8630,"uid":0,"gid":0}]'
  else
    jq_set "$CFG" '.linux.resources.devices += [{"allow":true,"type":"c","major":10,"minor":232,"access":"rwm"},{"allow":true,"type":"c","major":10,"minor":200,"access":"rwm"}]'
    jq_set "$CFG" '.linux.devices+=[{"path":"/dev/net/tun","type":"c","major":10,"minor":200,"fileMode":8630,"uid":0,"gid":0},{"path":"/dev/kvm","type":"c","major":10,"minor":232,"fileMode":8630,"uid":0,"gid":0}]'
  fi
  
  # For now, hardcode --security-opt=seccomp=unconfined;
  # later, we can work out the minimal seccomp permissions required.
  jq_set "$CFG" '.linux.seccomp |= empty'
  
  # CONFIGURE MEMORY
  # Set /dev/shm to RUNCVM_MEM_SIZE env var, or to default
  # - it should be large enough to support VM memory
  RUNCVM_MEM_LIMIT=$(jq_get "$CFG" '.linux.resources.memory.limit')
  log "RUNCVM_MEM_LIMIT=$RUNCVM_MEM_LIMIT"
  if [ "$RUNCVM_MEM_LIMIT" != "null" ]; then
    RUNCVM_MEM_SIZE="$(( $RUNCVM_MEM_LIMIT/1024/1024 ))"
  else
    RUNCVM_MEM_SIZE="$RUNCVM_MEM_SIZE_DEFAULT"
  fi
  log "RUNCVM_MEM_SIZE=${RUNCVM_MEM_SIZE}M"
  set_config_env "RUNCVM_MEM_SIZE" "${RUNCVM_MEM_SIZE}M"

  RUNCVM_HUGETLB=$(get_config_env "RUNCVM_HUGETLB")
  if [ "$RUNCVM_HUGETLB" != "1" ]; then
    jq_set "$CFG" --arg size "${RUNCVM_MEM_SIZE}M" '( .mounts[] | select(.destination == "/dev/shm") ) = {"destination": "/dev/shm","type": "tmpfs","source": "shm","options": ["nosuid","noexec","nodev","mode=1777","size=" + $size]}'
  # else
    # --shm-size applies; default 64m.
  fi

  # Set the container memory limit with a reasonable additional reserve to allow for
  # QEMU, virtiofsd, dnsmasq and other container memory footprint.
  RUNCVM_MEM_TOTAL_BYTES="$((($RUNCVM_MEM_SIZE + $RUNCVM_CONTAINER_MEM_OVERHEAD)*1024*1024))"
  jq_set "$CFG" --arg size "$RUNCVM_MEM_TOTAL_BYTES" '.linux.resources.memory.limit = ($size | tonumber)'
  # Also set swap limit to same value to avoid "memory+swap limit >= memory limit" error on cgroup v2
  jq_set "$CFG" --arg size "$RUNCVM_MEM_TOTAL_BYTES" '.linux.resources.memory.swap = ($size | tonumber)'

  # Add non-default capabilities needed by:
  # - Docker: CAP_NET_ADMIN
  # - Podman: CAP_NET_ADMIN, CAP_NET_RAW, CAP_MKNOD, CAP_AUDIT_WRITE
  for field in bounding effective permitted
  do
    jq_set "$CFG" --arg field "bounding" '.process.capabilities[$field] |= (.+ ["CAP_NET_ADMIN","CAP_NET_RAW","CAP_MKNOD","CAP_AUDIT_WRITE"] | unique)'
  done
  
  # Filter for RUNCVM_SYS_ADMIN=1
  RUNCVM_SYS_ADMIN=$(get_config_env "RUNCVM_SYS_ADMIN")
  if [ "$RUNCVM_SYS_ADMIN" = "1" ]; then
    # TODO use 'unique'
    jq_set "$CFG" '.process.capabilities.bounding += ["CAP_SYS_ADMIN"] | .process.capabilities.effective += ["CAP_SYS_ADMIN"] | .process.capabilities.permitted += ["CAP_SYS_ADMIN"]'
  fi

  # Save formatted config.json for debugging
  jq -r . <$CFG >/tmp/config.json-$$-2 2>/dev/null || true
  
elif [ "$COMMAND" = "exec" ]; then

  log_debug "Command: exec"

  # USAGE:
  #   runc exec [command options] <container-id> <command> [command options]  || -p process.json <container-id>
  #
  # PARSE 'exec' COMMAND OPTIONS
  # --console-socket value             path to an AF_UNIX socket which will receive a file descriptor referencing the master end of the console's pseudoterminal
  # --cwd value                        current working directory in the container
  # --env value, -e value              set environment variables
  # --tty, -t                          allocate a pseudo-TTY
  # --user value, -u value             UID (format: <uid>[:<gid>])
  # --additional-gids value, -g value  additional gids
  # --process value, -p value          path to the process.json
  # --detach, -d                       detach from the container's process
  # --pid-file value                   specify the file to write the process id to
  # --process-label value              set the asm process label for the process commonly used with selinux
  # --apparmor value                   set the apparmor profile for the process
  # --no-new-privs                     set the no new privileges value for the process
  # --cap value, -c value              add a capability to the bounding set for the process
  # --preserve-fds value               Pass N additional file descriptors to the container (stdio + $LISTEN_FDS + N in total) (default: 0)
  # --cgroup value                     run the process in an (existing) sub-cgroup(s). Format is [<controller>:]<cgroup>.
  # --ignore-paused                    allow exec in a paused container    
  while true
  do
    case "$1" in
      --console-socket|--cwd|--env|-e|--user|-u|--additional-gids|-g|--pid-file|--process-label|--apparmor|--cap|-c|--preserve-fds|--cgroup) shift; shift; continue; ;;
      --tty|-t|--detach|-d|--no-new-privs|--ignore-paused) shift; continue; ;;
      --process|-p) shift; PROCESS="$1"; continue; ;;
      *) break; ;;
    esac
  done

  # Allow user to enable debug logging
  if [ "$(get_process_env "$PROCESS" 'RUNCVM_RUNTIME_DEBUG' '0')" = "1" ]; then
    RUNCVM_LOG_LEVEL="DEBUG"
    CURRENT_LOG_LEVEL="${LOG_LEVELS[DEBUG]}"
  fi

  log_debug "Command line: $0 ${COMMAND_LINE[@]@Q}"
  log_debug "Command: exec process=$PROCESS"
  
  # Save formatted process.json
  jq -r . <$PROCESS >/tmp/process.json-$$-1

  ARG1=$(jq_get "$PROCESS" '.args[0]')
  if [ "$ARG1" = "---" ]; then
    jq_set "$PROCESS" 'del(.args[0])'
  else
    uidgid=$(jq_get "$PROCESS" '(.user.uid | tostring) + ":" + (.user.gid | tostring) + ":" + ((.user.additionalGids // []) | join(","))')
    cwd=$(jq_get "$PROCESS" '.cwd')
    hasHome=$(get_process_env_boolean "$PROCESS" 'HOME')
    wantsTerminal=$(jq_get "$PROCESS" '.terminal')

    jq_set "$PROCESS" \
      --arg exec "$RUNCVM_EXEC" \
      --arg uidgid "$uidgid" \
      --arg cwd "$cwd" \
      --arg hasHome "$hasHome" \
      --arg wantsTerminal "$wantsTerminal" \
      '.args |= [$exec, $uidgid, $cwd, $hasHome, $wantsTerminal] + .'

    # Force root (or whatever user qemu runs as)
    # Force cwd in the container to / 
    jq_set "$PROCESS" '.user = {"uid":0, "gid":0} | .cwd="/"'
  fi
  
  # Save formatted process.json for debugging
  jq -r . <$PROCESS >/tmp/process.json-$$-2 2>/dev/null || true

elif [ "$COMMAND" = "delete" ]; then

  log_debug "Command: delete"
  
  # USAGE:
  #   runc delete [command options] <container-id>
  #
  # PARSE 'delete' COMMAND OPTIONS
  # --force, -f  forcibly deletes the container if it is still running (uses SIGKILL)
  
  while true
  do
    case "$1" in
      --force|-f) shift; continue; ;;
      *) break; ;;
    esac
  done
  
  ID="$1"
  
  if [ -n "$ID" ]; then
    log "FIRECRACKER: Cleaning up NFS for container $ID"
    echo "[DEBUG] FIRECRACKER: Calling runcvm-nfsd stop-all for $ID" >&2
    
    # Stop unfsd daemon for this container
    if [ -x /opt/runcvm/scripts/runcvm-nfsd ]; then
      /opt/runcvm/scripts/runcvm-nfsd stop-all "$ID" 2>&1 || true
      log "FIRECRACKER: NFS cleanup complete for $ID"
    else
      log "WARNING: runcvm-nfsd not found at /opt/runcvm/scripts/runcvm-nfsd"
    fi
  fi
fi

log_debug "--- LOG ENDS ---"

exec /usr/bin/runc "${COMMAND_LINE[@]}"
